[
  {
    "id": "e88ff055-7040-4919-bf5c-17ffbd3227fc",
    "title": "thought-calibration-wu-etal.pdf",
    "pdf_url": "/static/uploads/e88ff055-7040-4919-bf5c-17ffbd3227fc/pdf/original.pdf",
    "extraction": {
      "pages": [
        {
          "figures": [],
          "page_number": 1,
          "text": "arXiv:2505.18404v1  [cs.LG]  23 May 2025\n<br><br>Thought calibration:\nEfficient and confident test-time scaling\n<br><br>Menghua Wu, Cai Zhou, Stephen Bates, Tommi Jaakkola\nDepartment of Computer Science\nMassachusetts Institute of Technology\nCambridge, MA 02319\n<br><br>Abstract\n<br><br>Reasoning large language models achieve im-\npressive test-time scaling by thinking for longer,\nbut this performance gain comes at significant\ncompute cost. Directly limiting test-time bud-\nget hurts overall performance, but not all prob-\nlems are equally difficult. We propose thought\ncalibration to decide dynamically when think-\ning can be terminated. To calibrate our decision\nrule, we view a language model\u2019s growing body\nof thoughts as a nested sequence of reasoning\ntrees, where the goal is to identify the point\nat which novel reasoning plateaus. We realize\nthis framework through lightweight probes that\noperate on top of the language model\u2019s hidden\nrepresentations, which are informative of both\nthe reasoning structure and overall consistency\nof response. Based on three reasoning language\nmodels and four datasets, thought calibration\npreserves model performance with up to a 60%\nreduction in thinking tokens on in-distribution\ndata, and up to 20% in out-of-distribution data.\n<br><br>1\nIntroduction\n<br><br>Test-time scaling presents a new paradigm for im-\nproving language model reasoning by expending\nlarge amounts of compute during inference (Ka-\nplan et al., 2020; Wei et al., 2022). Though the\nstrategies for eliciting reasoning vary \u2013 from large-\nscale reinforcement learning (Guo et al., 2025a)\nto explicit tree search (Zhang et al., 2024b,a) \u2013 a\ncommon effect is that language models improve\nby sampling substantially more tokens. This may\nresult in wasted compute on easy problems (Chen\net al., 2024; Sui et al., 2025), but naively limiting\nthe generation length leads to pronounced drops\nin accuracy (Muennighoff et al., 2025). This moti-\nvates early stopping strategies that reduce the infer-\nence budget without significantly degrading perfor-\nmance, and control the extent of impact, if perfor-\nmance must be compromised.\nNumerous methods have been proposed for\nteaching language models to be economical with\n<br><br>their token budgets (Han et al., 2024; Arora and\nZanette, 2025; Sui et al., 2025), or for identify-\ning opportune stopping points (Yang et al., 2025;\nZhang et al., 2025). While these methods demon-\nstrate strong empirical performance, they lack strict\nstatistical guarantees about when they could fail. In\nan orthogonal direction, conformal prediction has\nbeen adapted to equip language models with cali-\nbrated confidences about the quality or consistency\nof their generations (Mohri and Hashimoto, 2024;\nQuach et al., 2024; Rubin-Toles et al., 2025a,b;\nCherian et al., 2024). However, most of these al-\ngorithms operate through post-hoc filtering and\nrequire external LLM-based validation for scoring\nintermediate steps \u2013 rendering them unsuitable for\nactively terminating generation.\nIn this work, we jointly pursue an effective and\ncalibrated decision rule to determine when a lan-\nguage model can stop \u201cthinking.\u201d To do so, we\nintroduce the notion of a reasoning tree, where at\neach step of sampling, a language model either\nadds a new leaf, walks along the tree, or backtracks\nto a previous step. Notably, identifying when the\nthoughts have converged is equivalent to detecting\nwhen this reasoning tree stops growing. Inspired by\nthis concept, we approach early stopping as multi-\nple hypothesis testing problem. At each generation\nstep, we test whether the current tree is expected\nto change, based on the predictions of lightweight\nprobes over the language model\u2019s hidden represen-\ntations. Our algorithm is based on the Learn then\nTest framework (Angelopoulos et al., 2021), which\nprovides finite-sample, distribution-free guarantees\nfor controlling the risk of our decisions.\nWe evaluate this strategy, thought calibration,\nbased on its ability to guide efficient reasoning,\nand whether its decisions are well-calibrated. Our\nexperiments consider two empirical settings: we\nmay or may not have access to training and cali-\nbration data from the true test distribution. In the\nfirst setting, we train variants of thought calibration\n<br><br>1\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 2,
          "text": "using three reasoning lanugage models (DeepSeek-\nR1 distilled Qwen 32B and Llama 70B (Guo et al.,\n2025a; Yang et al., 2024; Grattafiori et al., 2024),\nQwQ 32B (Team, 2025)), evaluated on a helf-out\nsplit of s1K-1.1 (Muennighoff et al., 2025). Here,\nwe are able to halve the number of thinking tokens\nacross accuracy levels, with a maximum reduc-\ntion of 60%. Then we evaluate Qwen 32B-based\nthought calibration on three test datasets: AIME\n24, GPQA Diamond (Rein et al., 2024), MATH-\n500 (Lightman et al., 2023)). Though these datasets\nvary in format and difficult, thought calibration is\nstill able to reach up to a 20% reduction in think-\ning tokens, and in the worst case, is always as ef-\nficient as naive budget constraints. In summary,\nthis work has three main contributions.\n<br><br>1. We interpret LLM reasoning through the lens\nof an abstract reasoning tree, where the prob-\nlem of early exiting is equivalent to identify-\ning when this tree stops growing.\n<br><br>2. This view allows us to calibrate the decision\nrule for actively terminating generation.\n<br><br>3. Based on multiple language models and rea-\nsoning benchmarks, we provide empirical evi-\ndence that thought calibration is effective for\nefficient test-time scaling.\n<br><br>2\nBackground\n<br><br>2.1\nTest-time scaling\n<br><br>Efficient inference\nSince current reasoning mod-\nels are post-trained through reinforcement learn-\ning (Guo et al., 2025a), a number of works address\nthe overthinking problem (Sui et al., 2025) as part\nof the reinforcement learning process (Han et al.,\n2024; Arora and Zanette, 2025; Hou et al., 2025).\nOther works focus on the inference-time problem\nof predicting when a language model should stop\ngenerating (Yang et al., 2025; Zhang et al., 2025;\nMa et al., 2025). This papers falls under the latter\ncategory, which on a whole, is compatible with\nmethods that reduce a language model\u2019s verbosity\nduring post-training. Finally, another option is to\nachieve efficiency in terms of model architecture.\nSome works dynamically adapt compute cost (Lei,\n2021; Leviathan et al., 2023), while others employ\nonly a subset of all modules during sampling (Kim\nand Cho, 2021; Liu et al., 2022; Schuster et al.,\n2022).\nWhile these strategies operate over the\n<br><br>Transformer stack, rather than the generation se-\nquence length, many high-level ideas are broadly\napplicable to early exiting in our situation.\n<br><br>Self-consistency\nSelf-consistency\nhas\nbeen\nwidely used to provide a self-supervised form of\nconfidence during the sampling process (Wang\net al., 2022). These methods aim to improve the\nquality of generated samples, often in situations\nwhere multiple samples may be sequentially\ngenerated (Mitchell et al., 2022; Madaan et al.,\n2023; Shi et al., 2023; Weng et al., 2023; Guo\net al., 2025b).\nConsistency can also provide\nfeedback for reasoning-focused reinforcement\nlearning (Wang et al., 2024b).\nSeveral recent\nworks have observed that confidence scores can be\nprobed and calibrated from internal representations,\nto prioritize reasoning trajectories for subsequent\nruns (Li et al., 2024; Huang et al., 2025; Xie\net al., 2024) or for early exiting, similar to this\nwork (Zhang et al., 2025). Our key departure is\nthat we calibrate the decision rule to terminate\ngeneration, rather than the probabilistic outputs of\na probe. This reflects the online setting, where a\nprobe is used to actively guide generation, rather\nthan to filter trajectories post-hoc.\n<br><br>2.2\nConformal prediction and risk control\n<br><br>Conformal prediction quantifies the uncertainty in\nmachine learning models by generating set-valued\npredictions (Shafer and Vovk, 2008; Angelopoulos\nand Bates, 2021). These methods are distribution-\nfree and valid under finite samples, which makes\nthem particularly attractive in real-world applica-\ntions. Specifically, for an input x, a candidate out-\nput space Y, and a predetermined error level \u03f5,\nconformal prediction tests each potential outcome\ny \u2208 Y by evaluating the null hypothesis: \u201coutput\ny corresponds to input x.\u201d The final prediction set\nconsists of the outputs y for which this null hypoth-\nesis fails to be rejected, where the test statistic is\nknown as a nonconformity score. Split conformal\nprediction leverages a separate training set to learn\nthis nonconformity score (Vovk et al., 2005; Pa-\npadopoulos, 2008). The true outcome is included\nwith probability at least 1 \u2212 \u03f5, with guarantees that\nare typically marginal over draws of the test set and\nan exchangeable calibration set.\nIn the context of language modeling, conformal\nprediction has been adapted to calibrate the factu-\nality (Mohri and Hashimoto, 2024; Cherian et al.,\n2024), reasoning consistency (Rubin-Toles et al.,\n<br><br>2\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 3,
          "text": "2025a), and quality of generations (Quach et al.,\n2024; Qiu and Miikkulainen, 2024). Here, x may\nrepresent an input sequence of text, while y may be\na language model output. Of these works, Rubin-\nToles et al. (2025a) also introduces the idea of rea-\nsoning as coherency over a graph structure, based\non logical deducibility. However, this and other\nmethods are primarily designed for post-processing\ntext that has already been generated, and they rely\non external language models as scoring functions.\nAs a result, these approaches are not calibrated\nto be used as decision rules for iterative testing,\nand the latency required to compute nonconformity\nscores renders them unsuitable for early exiting.\nMore recently, the Learn then Test (LTT) frame-\nwork (Angelopoulos et al., 2021) extends the ideas\nin conformal prediction to control the risk of arbi-\ntrary loss functions, with guarantees over draws\nof the calibration set.\nOne application of LTT\nis to convert model outputs into a calibrated de-\ncision rule, by viewing hyperparameter selection\n(e.g. discretization thresholds) as multiple hypothe-\nsis testing. Our method and several works in early\nexiting are built atop the LTT framework. Quach\net al. (2024) calibrates a language model\u2019s sam-\npling of output sets, similar to this work. Their\ngoal is to generate sufficient outputs y until cer-\ntain admissibility criteria have been fulfilled, e.g.\ncorrectness and diversity of information. However,\nthe sampling process in Quach et al. (2024) is in-\nteractive, in the sense that each step requires an\nexternal verifier, and text may be added or removed\nat any point. As a result, this strategy is unsuit-\nable for providing online decisions about when to\nstop. Schuster et al. (2022) also leverages LTT\nto calibrate a stopping rule to exit from a Trans-\nformer stack. Their method operates on individual\ntokens, similar in spirit to applications like specu-\nlative decoding (Leviathan et al., 2023). Our focus\nis on large, coherent thoughts for reasoning, where\ntoken-level uncertainties are less informative.\n<br><br>3\nThought Calibration\n<br><br>Given an input x \u2208 X, a reasoning language model\ngenerates a series of thoughts y \u2208 Y, before syn-\nthesizing the final output z \u2208 Z. For example, x\nmay represent a math question; y is a sequence of\nreasoning steps; and z is the model\u2019s attempt at\nsolving the question (Figure 1A). Manipulating the\nbudget allocated to generating y directly impacts\nthe quality of z (Muennighoff et al., 2025), but as\n<br><br>the length of y increases, so too does the cost of\ninference. Our goal is to identify the point at which\ngrowing y no longer improves z.\nTo formalize these ideas, we introduce the no-\ntion of an abstract reasoning graph G, where nodes\nrepresent thoughts and directed edges represent en-\ntailment relationships (MacCartney and Manning,\n2014). This graph is rooted at x, the input question.\nNodes can be serialized into textual descriptions,\nand different paraphrases of the same idea repre-\nsent a single node. Where it is clear, we refer to\nthe abstract node and its textual representation in-\nterchangeably.\n<br><br>Definition 3.1. A reasoning trajectory z is a root-\nto-leaf walk in the reasoning graph G.\n<br><br>An arbitrary z need not be \u201ccomplete\u201d or \u201ccor-\nrect\u201d with respect to the original question x. We\nuse z\u2217 to denote a walk that starts at x and ends\nat the right answer, which we assume to be incon-\ntrovertible. G uniquely determines the set of all\nroot-to-leaf walks {z}, and thus, whether a lan-\nguage model has any chance of being correct in its\nfinal attempt.\n<br><br>Definition 3.2. A set of thoughts y is a walk, rooted\nat x, on the augmented graph G\u2032 in which every\nnode is connected to each of its ancestors.\n<br><br>At each stage of sampling, a large language\nmodel either adds a leaf to G (novel thought), or\ntakes one step in G\u2032 (backtracking or redundant\ngeneration). Let Gt be the reasoning graph at time\nt. If a language model terminates thinking at this\npoint, it is expected to answer correctly if there\nexists a path in Gt that yields z\u2217. Thus, it would be\nideal we could calibrate the language model such\nthat with high probability,\n<br><br>P (E [1[z\u2217 \u0338\u2208 Gt] \u2264 \u03b4]) \u2265 1 \u2212 \u03f5\n(1)\n<br><br>for some risk tolerance \u03b4 and error level \u03f5 \u2208 (0, 1).\nIn principle, a language model could enumerate the\nspace of graphs in a combinatorial search. How-\never, it is far from guaranteed that this graph can\nbe tractably found. Instead, we focus on the consis-\ntency between reasoning graphs.\n<br><br>Definition 3.3. Thoughts y and y\u2032 are consistent if\nthey can be represented by the reasoning graph G.\n<br><br>In particular, if a language model repeatedly\nrevisits a step to arrive at the same conclusion,\nor traverses the same ideas in a different order,\nthe resultant graph does not change (Figure 1C).\nLet yt := [y(i) . . . y(t)] and Gt be the in-progress\n<br><br>3\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 4,
          "text": "Input x \n<br><br>Given a rational number, write it as a fraction in lowest \nterms and calculate the product of the resulting \nnumerator and denominator. For how many rational \nnumbers between 0 and 1 will 20! be the result?\n<br><br>Thoughts y \n<br><br>First, a rational number between 0 and 1 can be \nwritten as a fraction a / b where 0 < a < b and a and b \nare coprime positive integers.\n<br><br>Reasoning step y(1)\n<br><br>The number with a < b is half of the total pairs, so \n256/2 =128. Hence 128 such numbers. Therefore, The \nanswer is 128, corresponding to 128.\n<br><br>Reasoning step y(T)\n<br><br>\u2026\n<br><br>Output z\n<br><br>The prime factors of 20! are 2, 3, 5, 7, 11, 13, 17, and 19. These \nare 8 distinct primes. For ab = 20! and gcd(a, b) = 1, each \nprime factor of 20! must be entirely to either a or b. This \nresults in 2^8 = 256 coprime pairs (a, b). Each pair (a, b) \nand (b, a) are distinct. To \ufb01nd pairs where a < b, we divide \nthe total number of pairs by 2s. The number of coprime \npairs (a, b) with a < b is 256/2 = 128.\n<br><br>x\n<br><br>y(1)\n<br><br>y(2)\n<br><br>Reasoning tree GT where T=4\n<br><br>a and b are coprime\n<br><br>20! has 8 distinct \nprime factors.\n<br><br>2^8 = 256\ny(4)\n<br><br>x\n<br><br>y(1)\n<br><br>y(2)\n<br><br>y(t)\n<br><br>\u2026\n<br><br>y(3)\n<br><br>(A) Reasoning language models scale at test \ntime through longer chains of thought, at \nsignificant compute cost.\n<br><br>(B) Terminating too early hurts model \nperformance due to under exploration.\n<br><br>20! is a \ufb01xed \nnumber, but \nhow is a \nrelated to b?\n<br><br>The answer is 256.\n<br><br>Potential reasoning trajectories:\n<br><br>z1 = [y(1), y(2), y(3)]\nz2 = [y(1), y(2), y(4)]\n<br><br><think>\n<br><br></think>\n<br><br>Thoughts\n<br><br>y = [y(1), y(2), y(3), y(4)]\n<br><br>y(3)\n<br><br>y(t-1)\n<br><br>When is E(risk of stopping now) < \u1e9f)?\n<br><br>256 / 2 = 128\nP(stop at t) = 0.8\n<br><br>2^8 = 256\nP(stop at t-1) = 0.4\nWait\u2026\nP(stop at t+1) = 0.85\n<br><br>Yes, 128.\nP(stop at T) = 0.9\n<br><br>(C) Our goal is to confidently decide when \nyt will be consistent with yT , based on when \nthe reasoning graph stops changing. \n<br><br>\u2026\n<br><br>20! has 8 \ndistinct \u2026\n<br><br>P(stop at 2) = 0.2\n<br><br>Reasoning tree GT\n<br><br>(Up to max budget)\n<br><br>Figure 1: Overview of the problem and our goal. Illustrated example based on s1K-1.1 (Muennighoff et al., 2025).\n<br><br>thoughts and reasoning graph after t steps, and\nlet T be the maximum inference budget (token or\nmodel limit). Instead of enforcing that Gt contains\nz\u2217, it is more reasonable to guarantee that\n<br><br>P (E [1[Gt \u0338= GT ] \u2264 \u03b4]) \u2265 1 \u2212 \u03f5.\n(2)\n<br><br>Due to the sequential nature of generation, Gt is\nalways a (not necessarily strict) subset of GT .\nWe now describe how we calibrate the decision\nrule for terminating language model generation\n(Section 3.1), and then introduces three strategies\nfor practically estimating the quantities described\nby Equations 1 and 2 (Section 3.2).\n<br><br>3.1\nCalibrating the stopping rule\n<br><br>Suppose we have a calibration dataset Dcal, which\ncontains exchangeable points {(xi, yi)}n\ni=1. Given\na new example x, let yt denote the language\nmodel\u2019s thoughts after t sampling steps, and let\nyT denote the maximum set of thoughts. Our goal\nis to find the smallest t that fulfills Equations 1 or\n2, based on the distribution of Dcal. During the\nsampling process, however, we do not know z\u2217\n<br><br>or GT , so we must estimate the quantities inside\nthe expectation using a surrogate function f. Here,\nDcal serves to calibrate f such that\n<br><br>P (E [R(yt) \u2264 \u03b4 | Dcal]) \u2265 1 \u2212 \u03f5\n(3)\n<br><br>where R is a bounded risk function associated with\nf. For example, f may be a linear probe on the\nhidden representations of thought steps y(i), and\n<br><br>its output may be a binary prediction. A potential\ndecision rule could take the form of a threshold \u03bb,\nwhere if f(y(t)) \u2265 \u03bb, we terminate thinking.\nSimilar to Schuster et al. (2022) and Quach et al.\n<br><br>(2024), we follow the Learn then Test framework to\nselect a valid set of \u03bbs that provide our desired guar-\nantees (Angelopoulos et al., 2021). On a high level,\nhyperparameter selection is viewed as a multiple\nhypothesis testing problem. Let \u039b be a finite set\nof configurations, where each \u03bbj \u2208 \u039b is associated\nwith the null hypothesis,\n<br><br>Hj : E[R(yt) > \u03b4].\n(4)\n<br><br>The set of valid \u039bvalid \u2286 \u039b is the set of \u03bbj for which\nwe fail to reject Hj. In particular, selecting the\nearliest stopping time is equivalent to identifying\nthe smallest \u03bb \u2208 \u039bvalid.\n<br><br>Theorem 3.4 (Adapted from theorem 1 in (An-\ngelopoulos et al., 2021)). Suppose pj is super-\nuniform under Hj for all j. Let A be a family-wise\nerror rate (FWER) controlling algorithm at level \u03f5.\nThen \u039bvalid = A(p1, . . . , pm) satisfies Equation 3.\n<br><br>Theorem\n3.4\nspecifies\nthat\nany\nFWER-\ncontrolling algorithm A can be used with an\nappropriate p-value to identify \u039bvalid.\nWhile\n<br><br>Angelopoulos et al. (2021) proposes several\nalgorithms to search over \u039b, we follow the fixed\nsequence testing method, since in principle, our\nrisks are expected to be monotonic (Gt \u2286 GT ).\nSpecifically, let \u039b = {\u03bb1, . . . , \u03bbm} be a de-\nscending grid of parameters. Intuitively, larger \u03bb\n<br><br>4\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 5,
          "text": "correspond to more permissive thresholds, e.g. al-\nlowing a language model to generate for longer.\n<br><br>1. For each j, we compute a valid p-value pj,\ne.g. the binomial tail bound p-value, follow-\ning (Quach et al., 2024):\n<br><br>pBT\n\u03bb\n:= P(Binom(n, \u03f5) \u2264 n ^Rn(\u03bb)).\n(5)\n<br><br>2. If pj \u2264 \u03f5, we reject Hj and continue. Oth-\nerwise, we return \u03bbj\u22121 as the smallest valid\nthreshold for error rate \u03f5.\n<br><br>This process yields the binarization threshold for\nf, where we stop generating when f(yt) \u2265 \u03bbj\u22121.\n<br><br>3.2\nEstimating empirical risk\n<br><br>On a high level, the surrogate function f should\nreflect the consistency of yt with expected future\ngenerations. Ideally, we would be able to access the\ngraphical structure of Gt, as any repetitions or re-\ndundant walks in yt would be immediately evident.\nHowever, since autoregressive language models\ngenerate left-to-right, without explicitly conform-\ning to any higher-level structure, we cannot operate\ndirectly over G. Instead, we introduce three ap-\nproaches for designing f in practice.\nWe first briefly consider the simple case sug-\ngested by Equation 1: if we terminate thinking\nnow, is the language model able to answer cor-\nrectly? That is, we could define\n<br><br>fcorrect(yt) := P(LLM is correct based on yt)\n(6)\n<br><br>Rcorrect(yt) := 1{LLM is correct} \u00b7 (1 \u2212 fcorrect(yt))\n<br><br>+ 1{LLM is wrong} \u00b7 fcorrect(yt).\n(7)\n<br><br>However, there are several drawbacks of this imple-\nmentation. By construction, the calibration dataset\nonly contain questions that can eventually be an-\nswered, which is not true in general. Though the\nspace of graphs is countable, it is unlikely that a\nlanguage model can efficiently explore the entire\nspace. In other words, the language model may\nrealistically never answer correctly. Thus, setting\n\u03bb = 1 is not guaranteed to be risk controlling. With\nthis definition of Rcorrect(y), calibrating based on\ncorrectness also requires supervised labels. While\nthis is not an issue on standard benchmarks, it is\nharder to obtain labels (user feedback) in practice.\nTo address these challenges, we introduce two\nadditional strategies for estimating graph consis-\ntency. First, a language model\u2019s final attempt z can\nbe viewed as a distillation of its overall reasoning\n<br><br>structure. Thus, we compare the language model\u2019s\nattempt zt after t steps, to the eventual attempt zT\nat the maximum reasoning budget. This yields\n<br><br>fconsistent(yt) := P(zt is the same as zT )\n(8)\n<br><br>Rconsistent(yt) := 1{consistent} \u00b7 (1 \u2212 fconsistent(yt))\n<br><br>+ 1{inconsistent} \u00b7 fconsistent(yt)\n(9)\n<br><br>These values can be determined even for intractable\nproblems, as long as the extended reasoning pro-\nduces no new insights, and does not require labels\nof correctness.\nFinally, any particular z only represents a single\nwalk through G. Due to stochasticity, two differing\nattempts could be sampled from the same graph,\nwhich is no longer changing. Towards this end,\nwe observed that language models often reiterate\nredundant information, after having reached the\ncorrect answer or the extent of its abilities. Prob-\ning for novelty should suffice to capture this phe-\nnomena. In practice, however, we found that the\nfollowing formulation was easier for our verifier to\nimplement, as checking for novelty involves long\ncontext reasoning over all previous thoughts, which\ncan be challenging (Wang et al., 2024a).\n<br><br>fnovel leaf(yt) := P(y(t)is leaf) \u00b7 (1 \u2212 P(y(t)is novel))\n(10)\n<br><br>Rnovel leaf(yt) := 1{LLM inconsistent} \u00b7 fnovel leaf(yt)\n<br><br>+ 1{LLM consistent} \u00b7 (1 \u2212 fnovel leaf(yt)).\n(11)\n<br><br>We reuse the labels for consistency due to ease of\nverification compared to novelty.\n<br><br>3.3\nImplementation details\n<br><br>To separate a reasoning trajectory y into individual\nsteps {y(i)}, we use sections delimited by \\n\\n,\nwhich also contain either wait or but. We ob-\nserved that individual tokens representations can\nvary significantly. Thus, each step uses the mean\nlast-layer representation of its tokens, followed by\ndimensionality reduction via PCA to d = 256.\nTo estimate each of quantities in Equations (6)\nto (11), we train linear probes on these step-level\nrepresentations.\nThe final probabilities are av-\neraged over a window of 10 steps for smooth-\nness, before calibration. For evaluation, we use\na grid of \u03f5 ranging from 0.05 to 0.5, with pre-\ncise thresholds selected to roughly match the token\nrange of baselines. During development, we ex-\nperimented with more complex architectures, e.g.\n<br><br>5\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 6,
          "text": "Transformer to predict leaves as a sequence la-\nbeling task (Appendix B.1). However, to avoid\noverfitting on our limited training set, we chose to\nfocus on simple and efficient linear probes. Con-\ncurrent work (Zhang et al., 2025) also finds that\nmodel confidence can often be extracted linearly.\nIn our experiments, we use three reasoning models:\nDeepSeek-R1 distilled Qwen 2.5 32B and Llama\n3.3 70B (Guo et al., 2025a; Grattafiori et al., 2024;\nYang et al., 2024), and QwQ 32B (Team, 2025).\nThe ground truth labels for these probes are ob-\ntained by prompting a separate language model\n(Qwen 3 32B). Correct: We truncate thinking tra-\njectories to desired lengths, append the <\\think>\ntoken, and prompt the language model for the\nfinal answer, which is compared to the ground\ntruth (Muennighoff et al., 2025). Consistent: The\nsame outputs can be used to check whether Gt is\nconsistent with GT , by comparing intermediate at-\ntempts zt to maximum budget attempt zT . Leaf:\nWe annotate whether each step y(i) is a leaf in G\nby asking a separate language model to identify\nwhether it makes an attempt to answer the origi-\nnal question x, regardless of correctness. Novel:\nWe provide a separate language model with all pre-\nvious thoughts y(1) . . . y(i\u22121) and ask whether the\nnew step y(i) provides additional information. All\nprompts can be found in Appendix A and were run\non 4 A6000 GPUs using vLLM (Kwon et al., 2023)\nand lmdeploy (Contributors, 2023).\nWe evaluate the correctness of all final attempts\nusing the GPT 4.1 API, between April 15, 2025 and\nMay 15, 2025. For datasets that have no ambiguity\n(multiple choice, numeric answers), we trimmed\nthe final attempts to 200 characters, to prevent the\nLLM from \u201ccheating\u201d by using additional thinking\nbudget after the </think> token.\n<br><br>4\nExperiments\n<br><br>4.1\nSettings\n<br><br>Datasets.\nOur experiments focus on efficient lan-\nguage model reasoning across tasks which vary in\ncontent, format, and difficulty. In particular, we\nleverage the following datasets.\ns1K-1.1 (Muennighoff et al., 2025) is a curated\ntraining set for distilling reasoning abilities through\ndata. This dataset contains 1000 difficult math and\nscience questions, along with thought trajectories\ngenerated by DeepSeek-R1 (Guo et al., 2025a). As\na proof of concept, we split the s1K-1.1 dataset into\ntraining, testing, and calibration (500, 50, 450, in\n<br><br>dataset order). We use the training set to develop\nour probes, which are calibrated on the calibration\nset and evaluated on the testing set.\nWe also consider three common reasoning\nbenchmarks solely for testing. AIME-24 is the\n2024 iteration of the American Invitational Mathe-\nmatics Examination.1 This dataset contains math\nquestions whose answers take on integers be-\ntween 0 and 999. GPQA Diamond (Rein et al.,\n2024) is a PhD-level math and science reasoning\nbenchmark with multiple choice answers. MATH\n500 (Hendrycks et al., 2021; Lightman et al., 2023)\nis a curated subset of the MATH dataset, which\ncompetition math questions of various levels. Note\nthat while s1K-1.1 contains examples of both math-\nematical and scientific questions, the format and\nsubsequent reasoning patterns may vary. For ex-\nample, while s1K-1.1 is open-ended, the various\nchoices in GPQA must be compared. Thus, we\nview these three datasets as \u201cout of distribution\u201d\nfrom s1K-1.1, which is itself diverse.\n<br><br>Models.\nWe evaluate the three variants of thought\ncalibration: the supervised probe for correctness\n(Equation 6, Supervised); the consistency probe\n(Equation 8, Consistent); and the lack of novelty\nprobe (Equation 10, Novel Leaf). To contextualize\nour experimental results, we also consider a naive\nbudget-forcing baseline (Crop). Specifically, we\nset a fixed token budget for thinking (ranging from\n1024 to the full trajectory). Once the language\nmodel reaches this budget, thinking is immediately\nterminated and the model is prompted for a final\nanswer. This reflects both the practical use case\nof setting a limit on maximum generation tokens,\nand the strategy employed by Muennighoff et al.\n(2025). Finally, concurrent work has also observed\nthat probes for correctness (Zhang et al., 2025)\nare effective for early exiting. While this design\nmay not be valid for risk control in practice (LLMs\nare not guaranteed to ever answer correctly), the\nSupervised baseline is similar to this work.\n<br><br>4.2\nIn-distribution setting\n<br><br>We start with the case where we have access to sam-\nples x that are drawn from the same distribution\nas our eventual application. For example, a model\nprovider may possess typical examples of user data.\nOur goals are to lower the overall test-time bud-\nget while maintaining accuracy, and to control any\nnecessary drops in performance based on our pre-\n<br><br>1https://maa.org/maa-invitational-competitions/\n<br><br>6\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 7,
          "text": "200000\n400000\n<br><br>Tokens\n<br><br>0.3\n<br><br>0.5\n<br><br>0.7\n<br><br>Accuracy\n<br><br>DeepSeek Distilled Qwen 32B\n<br><br>200000\n400000\n<br><br>Tokens\n<br><br>0.3\n<br><br>0.5\n<br><br>0.7\n<br><br>DeepSeek Distilled Llama 70B\n<br><br>200000\n400000\n<br><br>Tokens\n<br><br>0.3\n<br><br>0.5\n<br><br>0.7\n<br><br>QwQ 32B\n<br><br>0.5\n1.0\n<br><br>1-\n<br><br>0.2\n<br><br>0.4\n<br><br>0.6\n<br><br>0.8\n<br><br>1.0\n<br><br>% of Full Accuracy\n<br><br>Calibration\n<br><br>Full\nCrop\nSupervised\nConsistent\nLeaf Novelty\n<br><br>Figure 2: On in-distribution data (held-out test split on s1K), variants of thought calibration achieve up to a 60%\nreduction in thinking tokens while maintaining full performance. Top right point: Complete DeepSeek-R1 thought\ntrajectory from (Muennighoff et al., 2025). Crop: Fix thinking budget at 512, 1024, 2048, 4096, and 8192 tokens.\nSupervised: exit based on predicted likelihood of correctness. Consistent, and Leaf Novelty: exit based on predicted\nconsistency of answer or graph. Supervised is over confident, since the test set contains unsolvable problems.\n<br><br>100000\n150000\n200000\n<br><br>Tokens\n<br><br>0.4\n<br><br>0.5\n<br><br>0.6\n<br><br>0.7\n<br><br>Accuracy\n<br><br>AIME 24\n<br><br>250000 500000 750000\n<br><br>Tokens\n<br><br>0.50\n<br><br>0.55\n<br><br>0.60\n<br><br>0.65\n<br><br>GPQA Diamond\n<br><br>0.5\n1.0\n<br><br>Tokens\n1e6\n<br><br>0.75\n<br><br>0.80\n<br><br>0.85\n<br><br>0.90\n<br><br>0.95\nMATH 500\n<br><br>0.5\n1.0\n<br><br>1-\n<br><br>0.2\n<br><br>0.4\n<br><br>0.6\n<br><br>0.8\n<br><br>1.0\n<br><br>% of Full Accuracy\n<br><br>Calibration\n<br><br>Full\nCrop\nSupervised\nConsistent\n<br><br>Figure 3: We applied thought calibration probes for DeepSeek-distilled Qwen-2.5 32B on standard math and science\nbenchmarks, which may be out-of-distribution compared to the training and calibration sets, drawn from s1K. We\nachieve up to a 20% reduction in thinking tokens. While Consistent generally remains below the predetermined\nerror rates, Supervised is overconfident (as expected).\n<br><br>determined error levels. In Figure 2, we observe\nthat these probes are able to reduce the number of\nthinking tokens by over half for all three mod-\nels, with minimal impact to overall performance.\nWith respect to calibration, the Supervised probe\nis quite poorly calibrated, especially at lower val-\nues of \u03f5. All other probes are well calibrated at\n\u03f5 < 0.1, though variance is higher outside of this\nrange. This may be due to distribution shift, result-\ning from the small test split (to maximize training\nand calibration data for subsequent evaluations).\n<br><br>4.3\nGeneralization setting\n<br><br>Next, we consider the case in which the data we\nhave is related, but not drawn from the same dis-\ntribution as our eventual application. To emulate\nthis setting, we apply the supervised and consis-\ntent Qwen 32B probes, developed on the s1K-1.1\ndataset, to common reasoning benchmarks (Rein\net al., 2024; Lightman et al., 2023). Overall, we\nare able to improve (AIME 24, GPQA) or match\n(MATH 500) the efficiency of the budget forcing\nbaseline \u2013 even achieving slight gains in perfor-\nmance on AIME 24, perhaps by trimming dis-\ntracting thoughts (Figure 4). Notably, even though\nthe Supervised probe had access to more informa-\ntion (ground truth answers), the Consistent probe\n<br><br>consistently generalizes better, both in terms of\nefficiency and calibration. Here, the Consistent\nprobe fulfills the theoretical guarantees, while the\nSupervised probe remains over-confident.\n<br><br>4.4\nAdditional analysis\n<br><br>Figure 4 illustrates that thought calibration probes\nprioritizes the termination of problems which can-\nnot be solved, even at full budget \u2013 perhaps hint-\ning that the language model may have been stuck\nin a cycle of reasoning, without novel progress.\nCompared to the naive cropping strategy, thought\ncalibration\u2019s input-dependent decision also demon-\nstrate significant variance in the amount of tokens\nacross different problems.\nWe also examine a specific instance from our\ns1K-1.1 testing split in Figure 5 (s1K is a distil-\nlation dataset, so this diagram does not leak real\ntest examples). The language model reaches the\ncorrect answer after 38 steps (out of 48 steps). As\nthe model backtracks, the predicted consistency\n(with the expected final answer) drops; and as\nthe model returns to the answer, confidence in-\ncreases, higher than before. This reaffirms that\nself-consistency is indeed a powerful indication of\ncorrectness, both distilled into a predictive model,\nand over the course of sampling.\n<br><br>7\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 8,
          "text": "2048 4096 6144 8192\nMax Original Tokens\n<br><br>0\n<br><br>50\n<br><br>100\n<br><br>Percent Cropped\n<br><br>Budget=6144\n<br><br>2048 4096 6144 8192\nMax Original Tokens\n<br><br>0\n<br><br>50\n<br><br>100\n<br><br>Budget=4096\n<br><br>2048 4096 6144 8192\nMax Original Tokens\n<br><br>0\n<br><br>50\n<br><br>100\n<br><br>Budget=2048\n<br><br>2048 4096 6144 8192\nMax Original Tokens\n<br><br>0\n<br><br>50\n<br><br>100\n<br><br>Budget=1024\n<br><br>Original Score\n<br><br>0\n1\n<br><br>Strategy: Crop\n<br><br>2048 4096 6144 8192\nMax Original Tokens\n<br><br>0\n<br><br>50\n<br><br>100\n<br><br>Percent Cropped\n<br><br>=0.05\n<br><br>2048 4096 6144 8192\nMax Original Tokens\n<br><br>0\n<br><br>50\n<br><br>100\n<br><br>=0.1\n<br><br>2048 4096 6144 8192\nMax Original Tokens\n<br><br>0\n<br><br>50\n<br><br>100\n<br><br>=0.2\n<br><br>2048 4096 6144 8192\nMax Original Tokens\n<br><br>0\n<br><br>50\n<br><br>100\n<br><br>=0.3\n<br><br>Original Score\n<br><br>0\n1\n<br><br>Strategy: Consistent\n<br><br>Figure 4: Proportion of prompt tokens removed, for different thresholds, stratified by full thought length and\nwhether the original model was able to solve the problem. Top: Naive max token thresholding. Bottom: Consistency\ncalibration, DeepSeek-R1 distilled Qwen 32B, over GPQA Diamond. Cropping reduces token lengths uniformly,\nregardless of the input characteristics. Thought calibration has a preference for first trimming longer thoughts and\ncases where the language model tries but fails to make progress.\n<br><br>Okay, so I need to solve this problem: We have an increasing sequence: 3, 15, 24, 48, ... which\nare positive multiples of 3 that are one less than a perfect square. I need to find the remainder\nwhen the 1994th term of this sequence is divided by 1000.\n<br><br>Step 1 (0.104)\n<br><br>Hmm. Let me parse this. The terms are multiples of 3, and each term is one less than a perfect\nsquare [...] n must be either 1 or 2 mod 3. That means terms in the sequence correspond to n\nwhere n \u2261 1 or 2 mod 3.\n<br><br>Step 2 (0.067)\n<br><br>But computing 29922 mod 1000 is equivalent to [...] but since\n2992 = 2000 + 992 = 2000 + 992, but modulo 1000, 2992 \u2261 992 mod 1000, so 9922 \u2212 1\nmod 1000.\n<br><br>Step 37 (0.342)\n<br><br>Wait, first compute 9922 mod 1000, then subtract 1, then take\nmod 1000 again. [...]\nsubtract 1: 64 \u2212 1 = 63. Therefore, 9922 \u2212 1 mod 1000 = 63. Therefore the remainder is 63.\nSo answer is 63.\n<br><br>Step 38 (0.717)\n<br><br>But let me confirm because that seems straightforward. Wait: [...]\nStep 39 (0.646)\n<br><br>Wait: n(k) = (3k)/2 + 1 for even k. For even k = 1994, [...] Then term = 29922 \u2212 1. Then\nmod 1000 is (29922 \u2212 1) mod 1000.\nStep 40 (0.479)\n<br><br>But 2992 mod 1000 = 992, so 2992 \u2261 \u22128 mod 1000. Then (\u22128)2 = 64, then\n64 \u2212 1 = 63. Therefore mod 1000: 63. [...] Then (\u22128)2 = 64, then 64 \u2212 1 = 63. Therefore\nmod 1000: 63. Hence remainder is 63.\n<br><br>Step 41 (0.985)\n<br><br>Figure 5: DeepSeek-R1 distilled Llama 70B Consistency probe on s1K-1.1 example from our test split, where\ncolor intensity is proportional to P(consistent). The language model first reaches the correct answer in Step 38,\nbacktracks with lower confidence, and returns to the answer in Step 41.\n<br><br>8\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 9,
          "text": "5\nLimitations\n<br><br>There are several limitations of our work. Since\nour method is built atop the Learn then Test frame-\nwork (Angelopoulos et al., 2021), our theoretical\nguarantees are only valid over draws of the calibra-\ntion set. In practice, this means that the calibration\ndata must be sufficiently similar to the actual appli-\ncation. Furthermore, due to our small training and\ncalibration datasets, we implement our framework\nprimarily through linear probes. In Appendix B.1,\nwe found that more complex architectures may lead\nto slightly better performance in some cases, and\nthe gap is expected to be larger if more training data\ncan be gathered. We leave further investigations\nregarding the probe architecture to future work.\nFinally, this paper only addresses the problem of\nexiting early from reasoning. The broader question\nof how to calibrate the steering of reasoning mod-\nels remains unanswered, and is an interesting area\nfor further research.\n<br><br>References\n<br><br>Anastasios N Angelopoulos and Stephen Bates. 2021.\nA gentle introduction to conformal prediction and\ndistribution-free uncertainty quantification. arXiv\npreprint arXiv:2107.07511.\n<br><br>Anastasios N Angelopoulos, Stephen Bates, Em-\nmanuel J Cand\u00e8s, Michael I Jordan, and Lihua Lei.\n2021. Learn then Test: Calibrating predictive al-\ngorithms to achieve risk control.\narXiv preprint\narXiv:2110.01052.\n<br><br>Daman Arora and Andrea Zanette. 2025. Training lan-\nguage models to reason efficiently. arXiv preprint\narXiv:2502.04463.\n<br><br>Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He,\nJianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu,\nMengfei Zhou, Zhuosheng Zhang, and 1 others.\n2024.\nDo not think that much for 2+ 3=?\non\nthe overthinking of o1-like LLMs. arXiv preprint\narXiv:2412.21187.\n<br><br>John Cherian, Isaac Gibbs, and Emmanuel Candes.\n2024. Large language model validity via enhanced\nconformal prediction methods. Advances in Neural\nInformation Processing Systems, 37:114812\u2013114842.\n<br><br>LMDeploy Contributors. 2023. Lmdeploy: A toolkit\nfor compressing, deploying, and serving llm. https:\n//github.com/InternLM/lmdeploy.\n<br><br>Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,\nAbhinav Pandey, Abhishek Kadian, Ahmad Al-\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten,\nAlex Vaughan, and 1 others. 2024. The llama 3 herd\nof models. arXiv preprint arXiv:2407.21783.\n<br><br>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,\nRuoyu Zhang, Runxin Xu, Qihao Zhu, Shirong\nMa, Peiyi Wang, Xiao Bi, and 1 others. 2025a.\nDeepSeek-R1: Incentivizing reasoning capability in\nLLMs via reinforcement learning. arXiv preprint\narXiv:2501.12948.\n<br><br>Jiacheng Guo, Yue Wu, Jiahao Qiu, Kaixuan Huang,\nXinzhe Juan, Ling Yang, and Mengdi Wang. 2025b.\nTemporal consistency for llm reasoning process error\nidentification. arXiv preprint arXiv:2503.14495.\n<br><br>Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu\nZhao, Shiqing Ma, and Zhenyu Chen. 2024. Token-\nbudget-aware LLM reasoning.\narXiv preprint\narXiv:2412.18547.\n<br><br>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. In Thirty-\nfifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 2).\n<br><br>Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi\nQian, Jacob Andreas, and Shiyu Chang. 2025.\nThinkPrune:\nPruning long chain-of-thought of\nLLMs via reinforcement learning. arXiv preprint\narXiv:2504.01296.\n<br><br>9\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 10,
          "text": "Chengsong Huang, Langlin Huang, Jixuan Leng, Ji-\nacheng Liu, and Jiaxin Huang. 2025. Efficient test-\ntime scaling via self-calibration.\narXiv preprint\narXiv:2503.00031.\n<br><br>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\narXiv\npreprint arXiv:2001.08361.\n<br><br>Gyuwan Kim and Kyunghyun Cho. 2021.\nLength-\nadaptive transformer: Train once with length drop,\nuse anytime with search. In Joint Conference of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing, ACL-\nIJCNLP 2021, pages 6501\u20136511. Association for\nComputational Linguistics (ACL).\n<br><br>Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\nGonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-\ncient memory management for large language model\nserving with pagedattention. In Proceedings of the\nACM SIGOPS 29th Symposium on Operating Systems\nPrinciples.\n<br><br>Tao Lei. 2021. When attention meets fast recurrence:\nTraining language models with reduced compute. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\n<br><br>Yaniv Leviathan, Matan Kalman, and Yossi Matias.\n2023. Fast inference from transformers via spec-\nulative decoding. In International Conference on\nMachine Learning, pages 19274\u201319286. PMLR.\n<br><br>Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan,\nXinglin Wang, Bin Sun, Heda Wang, and Kan Li.\n2024. Escape sky-high cost: Early-stopping self-\nconsistency for multi-step reasoning. In The Twelfth\nInternational Conference on Learning Representa-\ntions.\n<br><br>Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harri-\nson Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\n2023. Let\u2019s verify step by step. In The Twelfth Inter-\nnational Conference on Learning Representations.\n<br><br>Zhuang Liu, Zhiqiu Xu, Hung-Ju Wang, Trevor Dar-\nrell, and Evan Shelhamer. 2022. Anytime dense pre-\ndiction with confidence adaptivity. In International\nConference on Learning Representations.\n<br><br>Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs,\nSewon Min, and Matei Zaharia. 2025. Reasoning\nmodels can be effective without thinking.\narXiv\npreprint arXiv:2504.09858.\n<br><br>Bill MacCartney and Christopher D Manning. 2014.\nNatural logic and natural language inference. In Com-\nputing Meaning: Volume 4, pages 129\u2013147. Springer.\n<br><br>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nand 1 others. 2023. Self-refine: Iterative refinement\nwith self-feedback. Advances in Neural Information\nProcessing Systems, 36:46534\u201346594.\n<br><br>Eric Mitchell, Joseph Noh, Siyan Li, Will Armstrong,\nAnanth Agarwal, Patrick Liu, Chelsea Finn, and\nChristopher D Manning. 2022.\nEnhancing self-\nconsistency and performance of pre-trained language\nmodels through natural language inference. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1754\u2013\n1768.\n<br><br>Christopher Mohri and Tatsunori Hashimoto. 2024.\nLanguage models with conformal factuality guaran-\ntees. In Proceedings of the 41st International Con-\nference on Machine Learning, ICML\u201924. JMLR.org.\n<br><br>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xi-\nang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke\nZettlemoyer, Percy Liang, Emmanuel Cand\u00e8s, and\nTatsunori Hashimoto. 2025. s1: Simple test-time\nscaling. arXiv preprint arXiv:2501.19393.\n<br><br>Harris Papadopoulos. 2008. Inductive conformal pre-\ndiction: Theory and application to neural networks.\nIn Tools in artificial intelligence. Citeseer.\n<br><br>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011.\nScikit-learn: Machine learning in\nPython.\nJournal of Machine Learning Research,\n12:2825\u20132830.\n<br><br>Xin Qiu and Risto Miikkulainen. 2024. Semantic den-\nsity: Uncertainty quantification for large language\nmodels through confidence measurement in semantic\nspace. In Advances in Neural Information Processing\nSystems, volume 37, pages 134507\u2013134533.\n<br><br>Victor Quach, Adam Fisch, Tal Schuster, Adam Yala,\nJae Ho Sohn, Tommi S Jaakkola, and Regina Barzi-\nlay. 2024. Conformal language modeling. In The\nTwelfth International Conference on Learning Repre-\nsentations.\n<br><br>David Rein, Betty Li Hou, Asa Cooper Stickland, Jack-\nson Petty, Richard Yuanzhe Pang, Julien Dirani, Ju-\nlian Michael, and Samuel R Bowman. 2024. GPQA:\nA graduate-level Google-proof Q&A benchmark. In\nFirst Conference on Language Modeling.\n<br><br>Maxon Rubin-Toles, Maya Gambhir, Keshav Ramji,\nAaron Roth, and Surbhi Goel. 2025a. Conformal lan-\nguage model reasoning with coherent factuality. In\nThe Thirteenth International Conference on Learning\nRepresentations.\n<br><br>Maxon Rubin-Toles, Maya Gambhir, Keshav Ramji,\nAaron Roth, and Surbhi Goel. 2025b. Conformal lan-\nguage model reasoning with coherent factuality. In\n<br><br>10\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 11,
          "text": "The Thirteenth International Conference on Learning\nRepresentations.\n<br><br>Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani,\nDara Bahri, Vinh Tran, Yi Tay, and Donald Metzler.\n2022. Confident adaptive language modeling. Ad-\nvances in Neural Information Processing Systems,\n35:17456\u201317472.\n<br><br>Glenn Shafer and Vladimir Vovk. 2008. A tutorial on\nconformal prediction. Journal of Machine Learning\nResearch, 9(3).\n<br><br>Freda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed H Chi, Nathanael Sch\u00e4rli,\nand Denny Zhou. 2023. Large language models can\nbe easily distracted by irrelevant context. In Inter-\nnational Conference on Machine Learning, pages\n31210\u201331227. PMLR.\n<br><br>Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu\nZhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, An-\ndrew Wen, Shaochen Zhong, Hanjie Chen, and 1\nothers. 2025. Stop overthinking: A survey on ef-\nficient reasoning for large language models. arXiv\npreprint arXiv:2503.16419.\n<br><br>Qwen Team. 2025. QwQ-32B: Embracing the power of\nreinforcement learning.\n<br><br>Vladimir Vovk, Alexander Gammerman, and Glenn\nShafer. 2005.\nAlgorithmic learning in a random\nworld, volume 29. Springer.\n<br><br>Minzheng Wang, Longze Chen, Cheng Fu, Shengyi\nLiao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan\nXu, Lei Zhang, Run Luo, and 1 others. 2024a. Leave\nno document behind: Benchmarking long-context\nLLMs with extended multi-doc QA. CoRR.\n<br><br>Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai\nDai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.\n2024b. Math-shepherd: Verify and reinforce llms\nstep-by-step without human annotations. In Proceed-\nings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 9426\u20139439.\n<br><br>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models.\narXiv\npreprint arXiv:2203.11171.\n<br><br>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\nand 1 others. 2022. Chain-of-thought prompting elic-\nits reasoning in large language models. Advances\nin neural information processing systems, 35:24824\u2013\n24837.\n<br><br>Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He,\nShengping Liu, Bin Sun, Kang Liu, and Jun Zhao.\n2023. Large language models are better reasoners\nwith self-verification. In Findings of the Association\nfor Computational Linguistics: EMNLP 2023, pages\n2550\u20132575.\n<br><br>Zhihui Xie, Jizhou Guo, Tong Yu, and Shuai Li. 2024.\nCalibrating reasoning in language models with inter-\nnal consistency. arXiv preprint arXiv:2405.18711.\n<br><br>An Yang, Baosong Yang, Beichen Zhang, Binyuan\nHui, Bo Zheng, Bowen Yu, Chengyuan Li, Day-\niheng Liu, Fei Huang, Haoran Wei, and 1 others.\n2024.\nQwen2.5 technical report.\narXiv preprint\narXiv:2412.15115.\n<br><br>Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu,\nChenyu Zhu, Zheng Lin, Li Cao, and Weiping Wang.\n2025. Dynamic early exit in reasoning models. arXiv\npreprint arXiv:2504.15895.\n<br><br>Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Au-\nrojit Panda, Jinyang Li, and He He. 2025.\nRea-\nsoning models know when they\u2019re right: Probing\nhidden states for self-verification. arXiv preprint\narXiv:2504.05419.\n<br><br>Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue,\nYuxiao Dong, and Jie Tang. 2024a. Rest-mcts*: Llm\nself-training via process reward guided tree search.\nAdvances in Neural Information Processing Systems,\n37:64735\u201364772.\n<br><br>Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang\nLi, and Wanli Ouyang. 2024b.\nAccessing gpt-4\nlevel mathematical olympiad solutions via monte\ncarlo tree self-refine with llama-3 8b. arXiv preprint\narXiv:2406.07394.\n<br><br>11\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 12,
          "text": "A\nPrompts\n<br><br>The following prompt was used to force the model (DeepSeek-R1 distilled Qwen 32B and Llama 70B,\nQwQ 32B) to produce an answer after a fixed number of thinking steps. Following the recommendation\nof Guo et al. (2025a) and Team (2025), we do not include a system prompt. We apply the chat template to\nuser prompt before concatenating the \u201cin-progress\u201d thoughts. Adapted from (Muennighoff et al., 2025).\n<br><br><bos><User>\n<br><br>{question}\nPlease reason step by step, and put your final answer within \\\\boxed{{}}.\n<Assistant>\n<think>\n{thoughts}\n</think>\nFinal Answer:\n<br><br>The following prompt was used to obtain labels for P(correct) (Equation 6) using Qwen 3 32B. This\nprompt was also used to evaluate answers using GPT 4.1. Adapted from (Muennighoff et al., 2025).\n<br><br>You are an AI assistant for grading a science problem. The user will provide you with the question itself, the correct answer, and the\nstudent\u2019s attempt. Your job is to judge whether the attempt is correct by comparing it with the correct answer. If the correct answer\nis a number or choice, there should be no ambiguity, and you should directly compare the answer and the final result. If the attempt is\nincomplete, you should mark it as wrong. If the correct answer involves going through the entire reasoning process, you should judge the\nresult based on whether the reasoning process is correct, compared to correct answer.\nDo NOT try to solve the problem yourself. Only grade the attempt based on the correct answer.\nThe user will provide the attempt and the correct answer in the following format:\n# Problem\n{problem}\n## Correct answer\n{solution}\n## Student attempt\n{attempt}\nExplain your reasoning concisely, and end your response on a new line with only \"Yes\" or \"No\" (without quotes).\n<br><br>The following prompt was used to obtain labels for P(consistent) (Equation 8) using Qwen 3 32B.\n<br><br>You are an AI assistant for grading a science problem. The user will provide you with the question itself and two student attempts. Your\njob is to judge whether the two students arrive at the same answer. If question asks for a single numerical answer, there should be no\nambiguity, and you should directly compare the two answers. If the question asks for multiple parts, the two attempts are identical if only\nif all of the parts arrive at the same conclusion.\nDo NOT try to solve the problem yourself. Only grade whether the two attempts are the same.\nThe user will provide the problem and two attempts in the following format:\n# Problem\n{problem}\n## Attempt 1\n{attempt1}\n## Attempt 2\n{attempt2}\nExplain your reasoning concisely, and end your response on a new line with only \"Yes\" or \"No\" (without quotes).\n<br><br>The following prompt was used to obtain labels for P(leaf) (Equation 10) using Qwen 3 32B.\n<br><br>You are an AI assistant for parsing LLM outputs. The user will provide you with the question and an intermediate reasoning step. Your\njob is to judge whether the given step contains an attempt at a final answer.\nDo NOT attempt to solve the problem yourself. It does not matter if the answer is correct. Only comment on whether an attempt has\nbeen made.\nThe user will provide the problem and reasoning steps in the following format:\n# Problem\n{problem}\n# Reasoning step\n{reasoning step}\nExplain your reasoning, and end your response on a new line with only \"Yes\" or \"No\" indicating whether or the given step makes an\nattempt at providing the final answer.\n<br><br>The following prompt was used to obtain labels for P(novel) (Equation 10) using Qwen 3 32B.\n<br><br>You are an AI assistant for assessing the quality of logical reasoning. The user will provide you with the question and an incomplete\nattempt, consisting of a series of reasoning steps. Your job is to judge whether current step appears to provide additional information,\ncompared to the previous steps. If the current step is correct and novel, it is useful. If the current step is wrong or redundant, then it is\nnot useful.\nDo NOT try to solve the problem yourself. It does not matter if the attempt is not complete. Only comment on whether the current\nstep is useful.\nThe user will provide the problem and reasoning steps in the following format:\n# Problem\n<br><br>12\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 13,
          "text": "{problem}\n# Reasoning\n## step 1\n{reasoning step 1}\n## step 2\n{reasoning step 2}\n...\n## step k\n{reasoning step k}\n...\n## current step\n{current reasoning step}\nExplain your reasoning, and end your response on a new line with only \"Yes\" if the current step provides new information or \"No\"\notherwise (without quotes).\n<br><br>B\nImplementation details\n<br><br>B.1\nDesign and implementation of model probes\n<br><br>We tried several architectures, before deciding upon linear probes for simplicity and to avoid overfitting.\nThe differences in performance are not always consistent and the generalization gap is quite large (Table 1).\nSince our main focus is on calibration, and it requires significant compute to produce and evaluate scaling\ncurves, we consider more exhaustive exploration of alternate architectures as future work.\n<br><br>MLP\nThe input is a single representation h(t) corresponding to single reasoning step y(t), and the output\nis a binary label \u2208 {0, 1}. We train until AUC fails to improve for 10 epochs on 10% of the training set\n(randomly sampled). We report the best calibration set performance of the following hyperparameters.\nWe use the sklearn defaults otherwise (Pedregosa et al., 2011).\n<br><br>\u2022 Layers: 1, 2\n<br><br>\u2022 FFN dimension: 32, 64, 128\n<br><br>Transformer\nThe input is a sequence of representations, h(1) . . . h(t) corresponding to thoughts yt =\ny(1) . . . y(t). The output is either a binary label \u2208 {0, 1} for P(correct) and P(consistent), or a sequence\nof labels \u2208 {0, 1}t for P(novel) and P(leaf). For the former, we treat the embeddings as a set (i.e. if any\nrepresentation is sufficient to answer correctly, or be consistent). For the latter, we apply a left-to-right\ncausal attention mask during training, and we use sinusoidal positional encodings to encode the index of\neach reasoning step. We report the best calibration set performance of the following hyperparameters. In\ncontrast to the linear and MLP models, we find that the Transformer performs best if we do not apply\nPCA and instead operate over the original model dimension.\n<br><br>\u2022 Layers: 1, 2\n<br><br>\u2022 Model dimension: 16, 32, 64\n<br><br>\u2022 FFN dimension: 64, 128\n<br><br>\u2022 Number of heads: 4, 8\n<br><br>\u2022 Epochs: 5, 10\n<br><br>B.2\nLLM experiments\n<br><br>We ran DeepSeek-R1 distilled Qwen 2.5 32B and Llama 70B, and QwQ 32B using lmdeploy (Contributors,\n2023) with recommended defaults for each model. lmdeploy natively supports the saving of last layer\nrepresentations, so it was used for almost all experiments. We ran Qwen 3 32B using vLLM (Kwon et al.,\n2023) due to early support. Due to computational constraints, we report the mean over a single run.\nWe downloaded all model weights from transformers between April 1, 2025 and May 1, 2025.\n<br><br>13\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 14,
          "text": "Table 1: Probe architecture performance on s1K-1.1 train and calibration splits. Metric: Binary AUROC.\n<br><br>Linear\nMLP\nTransformer\n<br><br>Model\nQuantity\nTrain\nCal\nTrain\nCal\nTrain\nCal\n<br><br>DeepSeek-R1\ndistilled Qwen\n2.5 32B\n<br><br>P(correct)\n0.936\n0.788\n0.990\n0.779\n0.994\n0.760\nP(consistent)\n0.919\n0.788\n0.994\n0.747\n0.991\n0.773\nP(leaf)\n0.868\n0.839\n0.936\n0.815\n0.933\n0.852\nP(novel)\n0.874\n0.686\n0.980\n0.692\n0.896\n0.774\n<br><br>DeepSeek-R1\ndistilled Llama\n3.3 70B\n<br><br>P(correct)\n0.937\n0.765\n0.987\n0.746\n0.991\n0.803\nP(consistent)\n0.921\n0.745\n0.994\n0.743\n0.993\n0.748\nP(leaf)\n0.864\n0.819\n0.970\n0.802\n0.923\n0.848\nP(novel)\n0.872\n0.686\n0.981\n0.702\n0.915\n0.774\n<br><br>QwQ 32B\n<br><br>P(correct)\n0.943\n0.848\n0.986\n0.838\n0.948\n0.848\nP(consistent)\n0.950\n0.699\n0.988\n0.704\n0.939\n0.756\nP(leaf)\n0.869\n0.840\n0.942\n0.822\n0.913\n0.857\nP(novel)\n0.876\n0.677\n0.952\n0.690\n0.895\n0.792\n<br><br>C\nAdditional analysis\n<br><br>Figure 6 illustrates the early exit probabilities for each of the three probes. The supervised (\u201ccorrect\u201d)\nprobe reaches high exit probabilities the fastest, but it is also the most overconfident (Figure 2D).\n<br><br>0\n50\n100\n150\n<br><br>Reasoning steps\n<br><br>0.0\n<br><br>0.5\n<br><br>1.0\n<br><br>P(correct)\n<br><br>0\n50\n100\n150\n<br><br>Reasoning steps\n<br><br>0.5\n<br><br>1.0\n<br><br>P(consistent)\n<br><br>0\n50\n100\n150\n<br><br>Reasoning steps\n<br><br>0.00\n<br><br>0.01\n<br><br>0.02\n<br><br>0.03\n<br><br>P(no novel leaves)\n<br><br>Figure 6: Likelihoods of thought calibration probes over s1K-1.1 test set (10 examples). The \u201cNo Leaf\u201d variant is\nthe least monotonic. This could potentially indicate that after reaching the answer, the language model explores new\nknowledge that is irrelevant to the task.\n<br><br>14\n<br><br>"
        }
      ]
    },
    "annotations": [],
    "citations": [
      {
        "DOI": "",
        "URL": "https://arxiv.org/pdf/2505.18404v1.pdf",
        "abstract": "",
        "arxiv_id": "2505.18404v1",
        "author": "",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2505.18404v1.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "arXiv:2505.18404v1"
        ],
        "year": null
      },
      {
        "DOI": "",
        "URL": "https://arxiv.org/pdf/2107.07511.pdf",
        "abstract": "Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed to provide the reader a working understanding of conformal prediction and related distribution-free uncertainty quantification techniques with one self-contained document. We lead the reader through practical theory for and examples of conformal prediction and describe its extensions to complex machine learning tasks involving structured outputs, distribution shift, time-series, outliers, models that abstain, and more. Throughout, there are many explanatory illustrations, examples, and code samples in Python. With each code sample comes a Jupyter notebook implementing the method on a real-data example; the notebooks can be accessed and easily run using our codebase.",
        "arxiv_id": "2107.07511",
        "author": "Anastasios Nikolas Angelopoulos, Stephen Bates",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2107.07511.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification"
        ],
        "year": 2021
      },
      {
        "DOI": "10.1214/24-aoas1998",
        "URL": "https://arxiv.org/pdf/2110.01052.pdf",
        "abstract": "We introduce a framework for calibrating machine learning models so that their predictions satisfy explicit, finite-sample statistical guarantees. Our calibration algorithms work with any underlying model and (unknown) data-generating distribution and do not require model refitting. The framework addresses, among other examples, false discovery rate control in multi-label classification, intersection-over-union control in instance segmentation, and the simultaneous control of the type-1 error of outlier detection and confidence set coverage in classification or regression. Our main insight is to reframe the risk-control problem as multiple hypothesis testing, enabling techniques and mathematical arguments different from those in the previous literature. We use the framework to provide new calibration methods for several core machine learning tasks, with detailed worked examples in computer vision and tabular medical data.",
        "arxiv_id": "2110.01052",
        "author": "Anastasios Nikolas Angelopoulos, Stephen Bates, E. Cand\u00e8s, Michael I. Jordan, Lihua Lei",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2110.01052.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Learn then Test: Calibrating Predictive Algorithms to Achieve Risk Control"
        ],
        "year": 2021
      },
      {
        "DOI": "10.48550/arXiv.2502.04463",
        "URL": "https://arxiv.org/pdf/2502.04463.pdf",
        "abstract": "Scaling model size and training data has led to great advances in the performance of Large Language Models (LLMs). However, the diminishing returns of this approach necessitate alternative methods to improve model capabilities, particularly in tasks requiring advanced reasoning. Large reasoning models, which leverage long chain-of-thoughts, bring unprecedented breakthroughs in problem-solving capabilities but at a substantial deployment cost associated to longer generations. Reducing inference costs is crucial for the economic feasibility, user experience, and environmental sustainability of these models. In this work, we propose to train large reasoning models to reason efficiently. More precisely, we use reinforcement learning (RL) to train reasoning models to dynamically allocate inference-time compute based on task complexity. Our method incentivizes models to minimize unnecessary computational overhead while maintaining accuracy, thereby achieving substantial efficiency gains. It enables the derivation of a family of reasoning models with varying efficiency levels, controlled via a single hyperparameter. Experiments on two open-weight large reasoning models demonstrate significant reductions in inference cost while preserving most of the accuracy.",
        "arxiv_id": "2502.04463",
        "author": "Daman Arora, Andrea Zanette",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2502.04463.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Training Language Models to Reason Efficiently"
        ],
        "year": 2025
      },
      {
        "DOI": "10.48550/arXiv.2412.21187",
        "URL": "https://arxiv.org/pdf/2412.21187.pdf",
        "abstract": "The remarkable performance of models like the OpenAI o1 can be attributed to their ability to emulate human-like long-time thinking during inference. These models employ extended chain-of-thought (CoT) processes, exploring multiple strategies to enhance problem-solving capabilities. However, a critical question remains: How to intelligently and efficiently scale computational resources during testing. This paper presents the first comprehensive study on the prevalent issue of overthinking in these models, where excessive computational resources are allocated for simple problems with minimal benefit. We introduce novel efficiency metrics from both outcome and process perspectives to evaluate the rational use of computational resources by o1-like models. Using a self-training paradigm, we propose strategies to mitigate overthinking, streamlining reasoning processes without compromising accuracy. Experimental results show that our approach successfully reduces computational overhead while preserving model performance across a range of testsets with varying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.",
        "arxiv_id": "2412.21187",
        "author": "Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2412.21187.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs"
        ],
        "year": 2024
      },
      {
        "DOI": "",
        "URL": "https://arxiv.org/pdf/2407.21783.pdf",
        "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.",
        "arxiv_id": "2407.21783",
        "author": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, A. Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony S. Hartshorn, Aobo Yang, Archi Mitra, A. Sravankumar, A. Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aur'elien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi\u00e8re, Bethany Biron, Binh Tang, Bobbie Chern, C. Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, C. Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab A. AlBadawy, E. Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriele Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, G. Mialon, Guanglong Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, J. V. D. Linde, Jennifer Billock, Jenny Hong, Jenya Lee, J. Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, J. Johnstun, Joshua Saxe, Ju-Qing Jia, Kalyan Vasuden Alwala, K. Upasani, Kate Plawiak, Keqian Li, Kenneth Heafield, Kevin R. Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuen-ley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, L. Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, M. Muzzi, Ma-hesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, M. Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Niko-lay Bashlykov, Nikolay Bogoychev, Niladri S. Chatterji, Olivier Duchenne, Onur cCelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasi\u0107, Peter Weng, Prajjwal Bhargava, P. Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, R. Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ron-nie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, S. Hosseini, Sa-hana Chennabasappa, Sanjay Singh, Sean Bell, S. Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, S. Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, S. Collot, Suchin Gururangan, S. Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Vir-ginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whit-ney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yiqian Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zhengxu Yan, Zhengxing Chen, Zoe Papakipos, Aaditya K. Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adi Gangidi, Adolfo Victoria, Ahuva Goldstand, A. Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, A. Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, B. Leonhardi, Po-Yao (Bernie) Huang, Beth Loyd, Beto de Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, B. Ni, Braden Hancock, Bram Wasti, Brandon Spence, B. Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Shang-Wen Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, E. Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, F. Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, F. Guzm\u2019an, Frank J. Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, G. Sizov, Guangyi Zhang, G. Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Han Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, J. Reizenstein, J. Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, J. McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kaixing(Kai) Wu, U. KamHou, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, K. Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, A. Lavender, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, M. Bhatt, M. Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthias Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, M. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, M. Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Mun-ish Bansal, N. Santhanam, Natascha Parks, Natasha White, Navy-ata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, O. Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pe-dro Rittner, Philip Bontrager, Pierre Roux, Piotr Doll\u00e1r, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, R. Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, S. Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Sheng Feng, Shenghao Lin, S. Zha, Shiva Shankar, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, S. Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, S. Gupta, Sung-Bae Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, V. Poenaru, Vlad T. Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2407.21783.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "The Llama 3 Herd of Models"
        ],
        "year": 2024
      },
      {
        "DOI": "10.48550/arXiv.2501.12948",
        "URL": "https://arxiv.org/pdf/2501.12948.pdf",
        "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
        "arxiv_id": "2501.12948",
        "author": "DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Jun-Mei Song, Ruoyu Zhang, R. Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiaoling Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, A. Liu, Bing Xue, Bing-Li Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, C. Deng, Chenyu Zhang, C. Ruan, Damai Dai, Deli Chen, Dong-Li Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, JingChang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. Cai, J. Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, K. Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, M. Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shao-Kang Wu, Tao Yun, Tian Pei, T. Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, W. Liang, Wenjun Gao, Wen-Xia Yu, Wentao Zhang, W. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, X. Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyu Jin, Xi-Cheng Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yi Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Y. Ou, Yuduan Wang, Yue Gong, Yu-Jing Zou, Yujia He, Yunfan Xiong, Yu-Wei Luo, Yu-mei You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yao Li, Yi Zheng, Yuchen Zhu, Yunxiang Ma, Ying Tang, Y. Zha, Yuting Yan, Z. Ren, Z. Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhen-guo Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zi-An Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, Zhen Zhang",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2501.12948.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
        ],
        "year": 2025
      },
      {
        "DOI": "10.48550/arXiv.2503.14495",
        "URL": "https://arxiv.org/pdf/2503.14495.pdf",
        "abstract": "Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency",
        "arxiv_id": "2503.14495",
        "author": "Jiacheng Guo, Yue Wu, Jiahao Qiu, Kaixuan Huang, Xinzhe Juan, Ling Yang, Mengdi Wang",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2503.14495.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Temporal Consistency for LLM Reasoning Process Error Identification"
        ],
        "year": 2025
      },
      {
        "DOI": "10.48550/arXiv.2412.18547",
        "URL": "https://arxiv.org/pdf/2412.18547.pdf",
        "abstract": "Reasoning is critical for large language models (LLMs) to excel in a wide range of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance LLM performance by decomposing problems into intermediate steps, they also incur significant overhead in token usage, leading to increased costs. We find that the reasoning process of current LLMs is unnecessarily lengthy and it can be compressed by including a reasonable token budget in the prompt, but the choice of token budget plays a crucial role in the actual compression effectiveness. We then propose a token-budget-aware LLM reasoning framework that dynamically adjusts the number of reasoning tokens based on the reasoning complexity of each problem. Experiments show that our method effectively reduces token costs in CoT reasoning with only a slight performance reduction, offering a practical solution to balance efficiency and accuracy in LLM reasoning. Code: https://github.com/GeniusHTX/TALE",
        "arxiv_id": "2412.18547",
        "author": "Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyun Zhao, Shiqing Ma, Zhenyu Chen",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2412.18547.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Token-Budget-Aware LLM Reasoning"
        ],
        "year": 2024
      },
      {
        "DOI": "10.48550/arXiv.2504.01296",
        "URL": "https://arxiv.org/pdf/2504.01296.pdf",
        "abstract": "We present ThinkPrune, a simple yet effective method for pruning the thinking length for long-thinking LLMs, which has been found to often produce inefficient and redundant thinking processes. Existing preliminary explorations of reducing thinking length primarily focus on forcing the thinking process to early exit, rather than adapting the LLM to optimize and consolidate the thinking process, and therefore the length-performance tradeoff observed so far is sub-optimal. To fill this gap, ThinkPrune offers a simple solution that continuously trains the long-thinking LLMs via reinforcement learning (RL) with an added token limit, beyond which any unfinished thoughts and answers will be discarded, resulting in a zero reward. To further preserve model performance, we introduce an iterative length pruning approach, where multiple rounds of RL are conducted, each with an increasingly more stringent token limit. We observed that ThinkPrune results in a remarkable performance-length tradeoff -- on the AIME24 dataset, the reasoning length of DeepSeek-R1-Distill-Qwen-1.5B can be reduced by half with only 2% drop in performance. We also observed that after pruning, the LLMs can bypass unnecessary steps while keeping the core reasoning process complete. Code is available at https://github.com/UCSB-NLP-Chang/ThinkPrune.",
        "arxiv_id": "2504.01296",
        "author": "Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2504.01296.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "ThinkPrune: Pruning Long Chain-of-Thought of LLMs via Reinforcement Learning"
        ],
        "year": 2025
      },
      {
        "DOI": "10.48550/arXiv.2503.00031",
        "URL": "https://arxiv.org/pdf/2503.00031.pdf",
        "abstract": "Increasing test-time computation is a straightforward approach to enhancing the quality of responses in Large Language Models (LLMs). While Best-of-N sampling and Self-Consistency with majority voting are simple and effective, they require a fixed number of sampling responses for each query, regardless of its complexity. This could result in wasted computation for simpler questions and insufficient exploration for more challenging ones. In this work, we argue that model confidence of responses can be used for improving the efficiency of test-time scaling. Unfortunately, LLMs are known to be overconfident and provide unreliable confidence estimation. To address this limitation, we introduce Self-Calibration by distilling Self-Consistency-derived confidence into the model itself. This enables reliable confidence estimation at test time with one forward pass. We then design confidence-based efficient test-time scaling methods to handle queries of various difficulty, such as Early-Stopping for Best-of-N and Self-Consistency with calibrated confidence. Experiments on three LLMs across six datasets demonstrate the effectiveness of our approach. Specifically, applying confidence-based Early Stopping to Best-of-N improves MathQA accuracy from 81.0 to 83.6 with a sample budget of 16 responses, indicating the efficacy of confidence-based sampling strategy at inference time.",
        "arxiv_id": "2503.00031",
        "author": "Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, Jiaxin Huang",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2503.00031.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Efficient Test-Time Scaling via Self-Calibration"
        ],
        "year": 2025
      },
      {
        "DOI": "",
        "URL": "https://arxiv.org/pdf/2001.08361.pdf",
        "abstract": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
        "arxiv_id": "2001.08361",
        "author": "J. Kaplan, Sam McCandlish, T. Henighan, Tom B. Brown, Benjamin Chess, R. Child, Scott Gray, Alec Radford, Jeff Wu, Dario Amodei",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2001.08361.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Scaling Laws for Neural Language Models"
        ],
        "year": 2020
      },
      {
        "DOI": "10.48550/arXiv.2504.09858",
        "URL": "https://arxiv.org/pdf/2504.09858.pdf",
        "abstract": "Recent LLMs have significantly improved reasoning capabilities, primarily by including an explicit, lengthy Thinking process as part of generation. In this paper, we question whether this explicit thinking is necessary. Using the state-of-the-art DeepSeek-R1-Distill-Qwen, we find that bypassing the thinking process via simple prompting, denoted as NoThinking, can be surprisingly effective. When controlling for the number of tokens, NoThinking outperforms Thinking across a diverse set of seven challenging reasoning datasets--including mathematical problem solving, formal theorem proving, and coding--especially in low-budget settings, e.g., 51.3 vs. 28.9 on ACM 23 with 700 tokens. Notably, the performance of NoThinking becomes more competitive with pass@k as k increases. Building on this observation, we demonstrate that a parallel scaling approach that uses NoThinking to generate N outputs independently and aggregates them is highly effective. For aggregation, we use task-specific verifiers when available, or we apply simple best-of-N strategies such as confidence-based selection. Our method outperforms a range of baselines with similar latency using Thinking, and is comparable to Thinking with significantly longer latency (up to 9x). Together, our research encourages a reconsideration of the necessity of lengthy thinking processes, while also establishing a competitive reference for achieving strong reasoning performance in low-budget settings or at low latency using parallel scaling.",
        "arxiv_id": "2504.09858",
        "author": "Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, Matei Zaharia",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2504.09858.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Reasoning Models Can Be Effective Without Thinking"
        ],
        "year": 2025
      },
      {
        "DOI": "10.48550/arXiv.2501.19393",
        "URL": "https://arxiv.org/pdf/2501.19393.pdf",
        "abstract": "Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending\"Wait\"multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1-32B exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https://github.com/simplescaling/s1",
        "arxiv_id": "2501.19393",
        "author": "Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Fei-Fei Li, Hanna Hajishirzi, Luke S. Zettlemoyer, Percy Liang, Emmanuel J. Cand\u00e8s, Tatsunori Hashimoto",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2501.19393.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "s1: Simple test-time scaling"
        ],
        "year": 2025
      },
      {
        "DOI": "10.48550/arXiv.2503.16419",
        "URL": "https://arxiv.org/pdf/2503.16419.pdf",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the\"overthinking phenomenon\". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking. Project website: https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs",
        "arxiv_id": "2503.16419",
        "author": "Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, Xia Hu",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2503.16419.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models"
        ],
        "year": 2025
      },
      {
        "DOI": "",
        "URL": "https://arxiv.org/pdf/2203.11171.pdf",
        "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
        "arxiv_id": "2203.11171",
        "author": "Xuezhi Wang, Jason Wei, D. Schuurmans, Quoc Le, Ed H. Chi, Denny Zhou",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2203.11171.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
        ],
        "year": 2022
      },
      {
        "DOI": "10.48550/arXiv.2405.18711",
        "URL": "https://arxiv.org/pdf/2405.18711.pdf",
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various reasoning tasks, aided by techniques like chain-of-thought prompting that elicits verbalized reasoning. However, LLMs often generate text with obvious mistakes and contradictions, raising doubts about their ability to robustly process and utilize generated rationales. In this work, we investigate reasoning in LLMs through the lens of internal representations, focusing on how these representations are influenced by generated rationales. Our preliminary analysis reveals that while generated rationales improve answer accuracy, inconsistencies emerge between the model's internal representations in middle layers and those in final layers, potentially undermining the reliability of their reasoning processes. To address this, we propose internal consistency as a measure of the model's confidence by examining the agreement of latent predictions decoded from intermediate layers. Extensive empirical studies across different models and datasets demonstrate that internal consistency effectively distinguishes between correct and incorrect reasoning paths. Motivated by this, we propose a new approach to calibrate reasoning by up-weighting reasoning paths with high internal consistency, resulting in a significant boost in reasoning performance. Further analysis uncovers distinct patterns in attention and feed-forward modules across layers, providing insights into the emergence of internal inconsistency. In summary, our results demonstrate the potential of using internal representations for self-evaluation of LLMs. Our code is available at github.com/zhxieml/internal-consistency.",
        "arxiv_id": "2405.18711",
        "author": "Zhihui Xie, Jizhou Guo, Tong Yu, Shuai Li",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2405.18711.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Calibrating Reasoning in Language Models with Internal Consistency"
        ],
        "year": 2024
      },
      {
        "DOI": "10.48550/arXiv.2412.15115",
        "URL": "https://arxiv.org/pdf/2412.15115.pdf",
        "abstract": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.",
        "arxiv_id": "2412.15115",
        "author": "Qwen An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan, Zekun Wang",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2412.15115.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Qwen2.5 Technical Report"
        ],
        "year": 2024
      },
      {
        "DOI": "10.48550/arXiv.2504.15895",
        "URL": "https://arxiv.org/pdf/2504.15895.pdf",
        "abstract": "Recent advances in large reasoning language models (LRLMs) rely on test-time scaling, which extends long chain-of-thought (CoT) generation to solve complex tasks. However, overthinking in long CoT not only slows down the efficiency of problem solving, but also risks accuracy loss due to the extremely detailed or redundant reasoning steps. We propose a simple yet effective method that allows LLMs to self-truncate CoT sequences by early exit during generation. Instead of relying on fixed heuristics, the proposed method monitors model behavior at potential reasoning transition points and dynamically terminates the next reasoning chain's generation when the model exhibits high confidence in a trial answer. Our method requires no additional training and can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments on 10 reasoning benchmarks (e.g., GSM8K, MATH-500, AMC, GPQA, AIME and LiveCodeBench) show that the proposed method is consistently effective on 11 cutting-edge reasoning LLMs of varying series and sizes, reducing the length of CoT sequences by an average of 19.1% to 80.1% while improving accuracy by 0.3% to 5.0%.",
        "arxiv_id": "2504.15895",
        "author": "Chenxu Yang, Q. Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Zheng Lin, Li Cao, Weiping Wang",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2504.15895.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Dynamic Early Exit in Reasoning Models"
        ],
        "year": 2025
      },
      {
        "DOI": "10.48550/arXiv.2504.05419",
        "URL": "https://arxiv.org/pdf/2504.05419.pdf",
        "abstract": "Reasoning models have achieved remarkable performance on tasks like math and logical reasoning thanks to their ability to search during reasoning. However, they still suffer from overthinking, often performing unnecessary reasoning steps even after reaching the correct answer. This raises the question: can models evaluate the correctness of their intermediate answers during reasoning? In this work, we study whether reasoning models encode information about answer correctness through probing the model's hidden states. The resulting probe can verify intermediate answers with high accuracy and produces highly calibrated scores. Additionally, we find models' hidden states encode correctness of future answers, enabling early prediction of the correctness before the intermediate answer is fully formulated. We then use the probe as a verifier to decide whether to exit reasoning at intermediate answers during inference, reducing the number of inference tokens by 24\\% without compromising performance. These findings confirm that reasoning models do encode a notion of correctness yet fail to exploit it, revealing substantial untapped potential to enhance their efficiency.",
        "arxiv_id": "2504.05419",
        "author": "Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Aurojit Panda, Jinyang Li, He He",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2504.05419.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Reasoning Models Know When They're Right: Probing Hidden States for Self-Verification"
        ],
        "year": 2025
      },
      {
        "DOI": "",
        "URL": "https://arxiv.org/pdf/2406.07394.pdf",
        "abstract": "This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.",
        "arxiv_id": "2406.07394",
        "author": "Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, Wanli Ouyang",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2406.07394.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "",
        "title": [
          "Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B"
        ],
        "year": 2024
      },
      {
        "DOI": "",
        "URL": "",
        "abstract": "",
        "arxiv_id": "",
        "author": "Learn",
        "link": [],
        "publisher": "",
        "raw_text": "Learn then Test: Calibrating predictive al- gorithms to achieve risk control. arXiv preprint arXiv:2110.01052. Daman Arora and Andrea Zanette. 2025. Training lan- guage models to reason efficiently. arXiv preprint arXiv:2502.04463. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, and 1 others.",
        "title": [
          "Learn then Test: Calibrating predictive al- gorithms to achieve risk control. arXiv preprint arXiv:2110.01052. Daman Arora and Andrea Zanette. 2025. Training lan- guage models to reason efficiently. a"
        ],
        "year": 2025
      },
      {
        "DOI": "10.48550/arXiv.2412.21187",
        "URL": "https://arxiv.org/pdf/2412.21187.pdf",
        "abstract": "The remarkable performance of models like the OpenAI o1 can be attributed to their ability to emulate human-like long-time thinking during inference. These models employ extended chain-of-thought (CoT) processes, exploring multiple strategies to enhance problem-solving capabilities. However, a critical question remains: How to intelligently and efficiently scale computational resources during testing. This paper presents the first comprehensive study on the prevalent issue of overthinking in these models, where excessive computational resources are allocated for simple problems with minimal benefit. We introduce novel efficiency metrics from both outcome and process perspectives to evaluate the rational use of computational resources by o1-like models. Using a self-training paradigm, we propose strategies to mitigate overthinking, streamlining reasoning processes without compromising accuracy. Experimental results show that our approach successfully reduces computational overhead while preserving model performance across a range of testsets with varying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.",
        "arxiv_id": "2412.21187",
        "author": "Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2412.21187.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "Do not think that much for 2+ 3=? on the overthinking of o1-like LLMs. arXiv preprint arXiv:2412.21187. John Cherian, Isaac Gibbs, and Emmanuel Candes.",
        "title": [
          "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs"
        ],
        "year": 2024
      },
      {
        "DOI": "",
        "URL": "",
        "abstract": "",
        "arxiv_id": "",
        "author": "Large",
        "link": [],
        "publisher": "",
        "raw_text": "Large language model validity via enhanced conformal prediction methods. Advances in Neural Information Processing Systems, 37:114812\u2013114842. LMDeploy Contributors. 2023. Lmdeploy: A toolkit for compressing, deploying, and serving llm. https: //github.com/InternLM/lmdeploy. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al- Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint ar",
        "title": [
          "Large language model validity via enhanced conformal prediction methods. Advances in Neural Information Processing Systems, 37:114812\u2013114842. LMDeploy Contributors. 2023. Lmdeploy: A toolkit for compr"
        ],
        "year": 2023
      },
      {
        "DOI": "10.1001/jama.2016.0287",
        "URL": "https://openalex.org/W2280404143",
        "abstract": "These updated definitions and clinical criteria should replace previous definitions, offer greater consistency for epidemiologic studies and clinical trials, and facilitate earlier recognition and more timely management of patients with sepsis or at risk of developing sepsis.",
        "arxiv_id": "",
        "author": "Mervyn Singer, Clifford S. Deutschman, Christopher W. Seymour, Manu Shankar\u2010Hari, Djillali Annane, Michael Bauer, Rinaldo Bellomo, Gordon R. Bernard, Jean\u2010Daniel Chiche, Craig M. Coopersmith",
        "link": [],
        "publisher": "",
        "raw_text": "Fast inference from transformers via spec- ulative decoding. In International Conference on Machine Learning, pages 19274\u201319286. PMLR. Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, and Kan Li.",
        "title": [
          "The Third International Consensus Definitions for Sepsis and Septic Shock (Sepsis-3)"
        ],
        "year": 2016
      },
      {
        "DOI": "",
        "URL": "",
        "abstract": "",
        "arxiv_id": "",
        "author": "Escape",
        "link": [],
        "publisher": "",
        "raw_text": "Escape sky-high cost: Early-stopping self- consistency for multi-step reasoning. In The Twelfth International Conference on Learning Representa- tions. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harri- son Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.",
        "title": [
          "Escape sky-high cost: Early-stopping self- consistency for multi-step reasoning. In The Twelfth International Conference on Learning Representa- tions. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Ha"
        ],
        "year": null
      },
      {
        "DOI": "10.1002/wps.20883",
        "URL": "https://openalex.org/W3197232629",
        "abstract": "As the COVID\u201019 pandemic has largely increased the utilization of telehealth, mobile mental health technologies \u2013 such as smartphone apps, vir\u00adtual reality, chatbots, and social media \u2013 have also gained attention. These digital health technologies offer the potential of accessible and scalable interventions that can augment traditional care. In this paper, we provide a comprehensive update on the overall field of digital psychiatry, covering three areas. First, we outline the relevance of recent technological advances to mental health research and care, by detailing how smartphones, social media, artificial intelligence and virtual reality present new opportunities for \u201cdigital phenotyping\u201d and remote intervention. Second, we review the current evidence for the use of these new technological approaches across different mental health contexts, covering their emerging efficacy in self\u2010management of psychological well\u2010being and early intervention, along with more nascent research supporting their use in clinical management of long\u2010term psychiatric conditions \u2013 including major depression; anxiety, bipolar and psychotic disorders; and eating and substance use disorders \u2013 as well as in child and adolescent mental health care. Third, we discuss the most pressing challenges and opportunities towards real\u2010world implementation, using the Integrated Promoting Action on Research Implementation in Health Services (i\u2010PARIHS) framework to explain how the innovations themselves, the recipients of these innovations, and the context surrounding innovations all must be considered to facilitate their adoption and use in mental health care systems. We conclude that the new technological capabilities of smartphones, artificial intelligence, social media and virtual reality are already changing mental health care in unforeseen and exciting ways, each accompanied by an early but promising evidence base. We point out that further efforts towards strengthening implementation are needed, and ",
        "arxiv_id": "",
        "author": "John Torous, Sandra Bucci, Imogen Bell, Lars Vedel Kessing, Maria Faurholt\u2010Jepsen, Pauline Whelan, Andr\u00e9 F. Carvalho, Matcheri S. Keshavan, Jake Linardon, Joseph Firth",
        "link": [],
        "publisher": "",
        "raw_text": "Let\u2019s verify step by step. In The Twelfth Inter- national Conference on Learning Representations. Zhuang Liu, Zhiqiu Xu, Hung-Ju Wang, Trevor Dar- rell, and Evan Shelhamer. 2022. Anytime dense pre- diction with confidence adaptivity. In International Conference on Learning Representations. Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. 2025. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858. Bill MacCartney and Christopher D M",
        "title": [
          "The growing field of digital psychiatry: current evidence and the future of apps, social media, chatbots, and virtual reality"
        ],
        "year": 2021
      },
      {
        "DOI": "10.7202/1013029ar",
        "URL": "https://openalex.org/W2012229299",
        "abstract": "South Africa\u2019s transformation to constitutionalism in 1994 saw the addition to a mixed legal system of a supreme constitution that requires all law to conform to its provisions, principles, and values. This new constitutional design was developed for the circumstances and modeled on existing liberal democratic constitutions, the most influential of which were Canadian and German. Adopted in 1993, the first constitution introduced the notion of the \u201cconstitutional state\u201d but being only a transitional document, it provided for the creation of a \u201cfinal\u201d constitution crafted in conformity with prescribed principles. The final constitution, adopted in 1996, made no mention of the \u201cconstitutional state\u201d, including instead the expression \u201crule of law\u201d. Since the constitutional principles laid down in 1993 referred to neither the German \u201c Rechtsstaat \u201d, nor Diceyan \u201crule of law\u201d, the replacement of the former term by the latter was permissible. The two constitutional texts did not, however, elaborate on these two terms. It was left to constitutional interpreters, especially the judiciary, to give meaning to these historically disconnected but conceptually related ideas. The result was a completely novel and pervasive constitutional doctrine. The judicial process of merging these notions may be described as \u201ccomparison by global assimilation\u201d.",
        "arxiv_id": "",
        "author": "Fran\u00e7ois Venter",
        "link": [],
        "publisher": "",
        "raw_text": "Christopher Mohri and Tatsunori Hashimoto. 2024. Language models with conformal factuality guaran- tees. In Proceedings of the 41st International Con- ference on Machine Learning, ICML\u201924. JMLR.org. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xi- ang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand\u00e8s, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393. Harris Papadopoulos. 2008. Inductive conformal pre- diction: The",
        "title": [
          "South Africa: A Diceyan Rechtsstaat?"
        ],
        "year": 2012
      },
      {
        "DOI": "10.48550/arxiv.0706.3188",
        "URL": "https://openalex.org/W2171585602",
        "abstract": "Conformal prediction uses past experience to determine precise levels of confidence in new predictions. Given an error probability $\u03b5$, together with a method that makes a prediction $\\hat{y}$ of a label $y$, it produces a set of labels, typically containing $\\hat{y}$, that also contains $y$ with probability $1-\u03b5$. Conformal prediction can be applied to any method for producing $\\hat{y}$: a nearest-neighbor method, a support-vector machine, ridge regression, etc. Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right $1-\u03b5$ of the time, even though they are based on an accumulating dataset rather than on independent datasets. In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in \"Algorithmic Learning in a Random World\", by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).",
        "arxiv_id": "",
        "author": "Glenn Shafer, Vladimir Vovk",
        "link": [],
        "publisher": "",
        "raw_text": "Confident adaptive language modeling. Ad- vances in Neural Information Processing Systems, 35:17456\u201317472. Glenn Shafer and Vladimir Vovk. 2008. A tutorial on conformal prediction. Journal of Machine Learning Research, 9(3). Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Sch\u00e4rli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. In Inter- national Conference on Machine Learning, pages 31210\u201331227. PMLR. Yang Sui, ",
        "title": [
          "A tutorial on conformal prediction"
        ],
        "year": 2007
      },
      {
        "DOI": "",
        "URL": "",
        "abstract": "",
        "arxiv_id": "",
        "author": "Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao",
        "link": [],
        "publisher": "",
        "raw_text": "Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao.",
        "title": [
          "Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao."
        ],
        "year": null
      },
      {
        "DOI": "10.1145/356810.356816",
        "URL": "https://openalex.org/W2051768896",
        "abstract": "article Free Access Share on The Hearsay-II Speech-Understanding System: Integrating Knowledge to Resolve Uncertainty Authors: Lee D. Erman USC/Information Sciences Institute, Marina del Rey, California USC/Information Sciences Institute, Marina del Rey, CaliforniaView Profile , Frederick Hayes-Roth The Rand Corporation, Santa Monica, California The Rand Corporation, Santa Monica, CaliforniaView Profile , Victor R. Lesser University of Massachusetts, Amherst, Massachusetts University of Massachusetts, Amherst, MassachusettsView Profile , D. Raj Reddy Carnegie-Mellon University, Pittsburgh, Pennsylvania Carnegie-Mellon University, Pittsburgh, PennsylvaniaView Profile Authors Info & Claims ACM Computing SurveysVolume 12Issue 2pp 213\u2013253https://doi.org/10.1145/356810.356816Published:01 June 1980Publication History 1,006citation2,698DownloadsMetricsTotal Citations1,006Total Downloads2,698Last 12 Months226Last 6 weeks28 Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account Save to BinderSave to BinderCreate a New BinderNameCancelCreateExport CitationPublisher SiteeReaderPDF",
        "arxiv_id": "",
        "author": "Lee D. Erman, Frederick Hayes\u2010Roth, Victor Lesser, D. Raj Reddy",
        "link": [],
        "publisher": "",
        "raw_text": "Large language models are better reasoners with self-verification. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2550\u20132575. Zhihui Xie, Jizhou Guo, Tong Yu, and Shuai Li. 2024. Calibrating reasoning in language models with inter- nal consistency. arXiv preprint arXiv:2405.18711. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Day- iheng Liu, Fei Huang, Haoran Wei, and 1 others.",
        "title": [
          "The Hearsay-II Speech-Understanding System: Integrating Knowledge to Resolve Uncertainty"
        ],
        "year": 1980
      },
      {
        "DOI": "10.48550/arXiv.2412.15115",
        "URL": "https://arxiv.org/pdf/2412.15115.pdf",
        "abstract": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.",
        "arxiv_id": "2412.15115",
        "author": "Qwen An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxin Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yi-Chao Zhang, Yunyang Wan, Yuqi Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu, Shanghaoran Quan, Zekun Wang",
        "link": [
          {
            "URL": "https://arxiv.org/pdf/2412.15115.pdf"
          }
        ],
        "publisher": "",
        "raw_text": "Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Zheng Lin, Li Cao, and Weiping Wang.",
        "title": [
          "Qwen2.5 Technical Report"
        ],
        "year": 2024
      },
      {
        "DOI": "",
        "URL": "",
        "abstract": "",
        "arxiv_id": "",
        "author": "Dynamic",
        "link": [],
        "publisher": "",
        "raw_text": "Dynamic early exit in reasoning models. arXiv preprint arXiv:2504.15895. Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Au- rojit Panda, Jinyang Li, and He He. 2025. Rea- soning models know when they\u2019re right: Probing hidden states for self-verification. arXiv preprint arXiv:2504.05419. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024a. Rest-mcts*: Llm self-training via process reward guided tree search. Advances in Neural Information Processing Systems, 37:64735\u20136",
        "title": [
          "Dynamic early exit in reasoning models. arXiv preprint arXiv:2504.15895. Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Au- rojit Panda, Jinyang Li, and He He. 2025. Rea- soning models know when they\u2019re"
        ],
        "year": 2025
      }
    ]
  }
]