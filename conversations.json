[
  {
    "id": "7e712e0b-9d32-493c-93d0-83005adf5f5e",
    "title": "elo uncovered.pdf",
    "pdf_url": "/static/uploads/40aec3b4-00e5-448b-8299-177a1fe22819.pdf",
    "extraction": {
      "pages": [
        {
          "figures": [],
          "page_number": 1,
          "text": "Elo Uncovered:\nRobustness and Best Practices in\nLanguage Model Evaluation\n<br><br>Meriem Boubdir\nCohere for AI\nmeri.boubdir@gmail.com\n<br><br>Edward Kim\nCohere\nedward@cohere.com\n<br><br>Beyza Ermis\nCohere for AI\nbeyza@cohere.com\n<br><br>Sara Hooker\nCohere for AI\nsarahooker@cohere.com\n<br><br>Marzieh Fadaee\nCohere for AI\nmarzieh@cohere.com\n<br><br>Abstract\n<br><br>In Natural Language Processing (NLP), the Elo rating system, originally designed for ranking play-\ners in dynamic games such as chess, is increasingly being used to evaluate Large Language Models\n(LLMs) through \u201cA vs B\u201d paired comparisons. However, while popular, the system\u2019s suitability for\nassessing entities with constant skill levels, such as LLMs, remains relatively unexplored. We study\ntwo fundamental axioms that evaluation methods should adhere to: reliability and transitivity. We\nconduct extensive evaluation of Elo behaviour, illustrating that individual Elo computations ex-\nhibit volatility and delving into the impact of varying the Elo rating system\u2019s hyperparameters. We\nshow that these axioms are not always satisfied raising questions about the reliability of current\ncomparative evaluations of LLMs. If the current use of Elo scores is intended to substitute the\ncostly head-to-head comparison of LLMs, it is crucial to ensure the ranking is as robust as possible.\nGuided by the axioms, our findings offer concrete guidelines for enhancing the reliability of LLM\nevaluation methods, suggesting a need for reassessment of existing comparative approaches.\n<br><br>1\nIntroduction\n<br><br>In the rapidly evolving field of Natural Language Processing (NLP), the task of accurately and\nreliably evaluating LLMs has become increasingly challenging (Liang et al., 2022; Chang et al.,\n2023; Srivastava et al., 2023; Kaddour et al., 2023; Pozzobon et al., 2023). Human feedback has\nemerged as an indispensable tool in this performance assessment process, serving as a qualitative\nmetric that captures nuances that automated scoring mechanisms often fail to address (Askell et al.,\n2021; Bai et al., 2022a;b; Srivastava et al., 2023; Ding et al., 2023; Dettmers et al., 2023).\n<br><br>These human-centered evaluations, highly valuable to the overall progress of the NLP field, typically\nadopt an \u201cA vs B\u201d comparative setup, turning evaluations into a zero-sum game between language\nmodels. This paired feedback structure (Zhao et al., 2023) naturally lends itself to the Elo rating\nsystem, originally designed for ranking chess players for better matchmaking (Elo, 1978). With\nElo rating system, we can integrate subjective human feedback into a structured rating system and\nassess the performance of language models.\n<br><br>1\n<br><br>arXiv:2311.17295v1  [cs.CL]  29 Nov 2023\n<br><br>"
        },
        {
          "figures": [
            {
              "ref": "Figure 1",
              "url": "/static/figures/475ad9d9-9b23-4c4a-8ad5-802e9bff7ae9.svg"
            }
          ],
          "page_number": 2,
          "text": "<a href='#' class='figure-link' data-figure-url='/static/figures/475ad9d9-9b23-4c4a-8ad5-802e9bff7ae9.svg'>Figure 1</a>: Impact of win probabilities and permutation sampling on Elo ratings: Com-\nparing Model A and Model B across three different win probabilities (Prob(A beats B) =\n{0.6, 0.55, 0.51}) with two levels of permutation sampling (Nperms = 1 and Nperms = 100). The\ntop row displays the observed win rates, the middle row illustrates Elo ratings with a single permu-\ntation, and the bottom row shows the mean and standard error of the mean (SEM) of Elo ratings\nacross 100 permutations.\n<br><br>The core principles of Elo rating have proven to be resilient and adaptable due to its dynamic\nadjustments, relative rating focus, consistency across skill levels, and simplicity and transparency.\nAs a result, the Elo system has found diverse applications, from predicting sports events outcomes\n(Binder & Findlay, 2009; Hvattum & Arntzen, 2010; Leitner et al., 2010; Wise, 2021), and facilitating\nmatchmaking in massively multiplayer online games like StarCraft ii and Dota\n(Ebtekar & Liu,\n2021; Reid; Liquipedia; ESL), to its recent use in the evaluation of LLMs (Askell et al., 2021; Bai\net al., 2022a;b; Srivastava et al., 2023; Ding et al., 2023; Dettmers et al., 2023; Wu et al., 2023; Lin &\nChen, 2023). However, to-date there has not been a comprehensive examination of the compatibility\nof Elo scores and LLMs.\n<br><br>Unlike dynamic competitors that evolve, LLMs have static capabilities and operate in a time-\nagnostic context. In this setting, not only are LLM evaluations unconstrained by a preset number of\nturns (unlike tournament timelines or predefined match sequences), but the ordering of matches can\nalso significantly influence the final Elo scores and, consequently, models rankings. This oversight\nis especially concerning, given the direct impact of Elo system rankings on both research directions\n<br><br>2\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 3,
          "text": "and real-world applications in NLP as well as its widespread adoption (Ye et al., 2023; Zheng et al.,\n2023; K\u00f6pf et al., 2023; Askell et al., 2021; Bai et al., 2022a;b; Srivastava et al., 2023; Ding et al.,\n2023; Dettmers et al., 2023; Wu et al., 2023; Lin & Chen, 2023).\n<br><br>This study aims to close this research gap by adopting an axiomatic approach and scrutinizing\nboth the reliability and limitations of the Elo rating system when applied to LLMs. We study two\nfundamental axioms that evaluation methods should adhere to: reliability and transitivity. Through\ntheoretical and empirical analyses grounded in collected human feedback data, our contributions\nprovide a comprehensive understanding of when and how to reliably employ the Elo system for LLM\nevaluation, thus offering valuable guidelines for researchers and practitioners in the NLP field.\n<br><br>We find that Elo ratings for LLMs are highly sensitive to the order of comparisons and the choice\nof hyperparameters. Moreover, desirable properties such as transitivity are not always guaranteed,\nand can be unreliable unless there is comprehensive human feedback data for all unique pairwise\ncomparisons among models in the feedback pool.\nThe sensitivity of Elo ratings becomes more\npronounced when dealing with models that exhibit similar performance levels. We illustrate the best\npractices for addressing Elo rating sensitivities by offering guidelines for hyperparameter selection\nand matchmaking scenarios.\n<br><br>Implications of our work As LLMs rapidly advance, evaluation leaderboards are gaining popu-\nlarity to assess the performance of newly introduced models using Elo scores. Elo can also be used\nin the learning framework of LLMs to produce a ranking of models and their outputs for prefer-\nence training. No research has explored the nuances of using Elo scores to compare LLMs, which,\nunlike chess, exhibit static capabilities and operate in a time-agnostic manner. We show that Elo\nrating does not always satisfy two critical axioms\u2014reliability and transitivity\u2014leading to rankings\nof models that are not accurate. Our research offers guidelines for reliable and robust implemen-\ntation of Elo scores when comparing LLMs. Deviation from our recommendations could result in\ninaccuracies when ranking LLMs, particularly in situations where model performances are closely\nmatched, and Elo differences are minimal (a common occurrence in many real-world scenarios).\n<br><br>2\nElo Algorithm Explained\n<br><br>We provide the mathematical formulation of the Elo algorithm, contextualized to the setting of\nLLM evaluation. In this formulation, let M be a set of models and each model i \u2208M is assigned\nan initial numerical rating Ri.\n<br><br>2.1\nExpected Score Computation\n<br><br>For a given paired match-up between two models A and B (A, B \u2208M), each with respective ratings\nRA and RB, the expected scores EA and EB are computed as:\n<br><br>EA =\n1\n<br><br>1 + 10(RB\u2212RA)/400\n(1a)\n<br><br>EB =\n1\n<br><br>1 + 10(RA\u2212RB)/400\n(1b)\n<br><br>In this context, the factor of 400 (Elo, 1978) precisely adjusts the sensitivity of the expected score\n<br><br>3\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 4,
          "text": "to differences in ratings. A 400-point advantage in ratings translates to a 10 : 1 odds in favor of\nthe higher-rated model, providing an interpretable metric for performance comparison. For evenly\nmatched models (RA = RB), both EA and EB equate to 0.5, reflecting a 50 : 50 win probability for\nboth models.\n<br><br>2.2\nRating Update Mechanism\n<br><br>Following each match, the Elo ratings are updated based on the observed outcome. The rating\nadjustment is dictated by the equation:\n<br><br>R\u2032\nA = RA + K(SA \u2212EA)\n(2)\n<br><br>Here, SA represents the actual score achieved by model A, which can take on either the value 0 or 1.\nThe K-factor serves as a variable hyperparameter to adapt the rate of change in rating to different\nscenarios.\n<br><br>3\nDesirable Properties of Elo\n<br><br>The objective of using Elo scores to rank models is to establish a comparative understanding of the\nperformance hierarchy among them. When incorporating a new model into an already ranked list,\nonly a limited number of pairwise annotations are required to determine its position in the ranking.\nThe ability to infer the relative performance of a model in comparison to all previous models in the\nlist relies on the robustness of the scoring method and the transitive property of the ranking. We\ndescribe this desirable characteristics for an evaluation framework using two axioms: transitivity\nand reliability.\n<br><br>3.1\nAxiom 1: Transitivity\n<br><br>A desirable property of any rating system is transitivity because it ensures consistency and logical\ncoherence in the way entities are ranked or rated. Transitivity in this context means that if player\nA beats player B, and player B beats player C, then player A is expected to beat player C. If the\nranking of large language models exhibits transitivity, we can deduce their comparative performance\nwithout the need for direct head-to-head evaluations between every pair of models. The central\nassumption in the development of various leaderboards for comparing language models is that the\nrankings adhere to the principle of transitivity (Zheng et al., 2023).\n<br><br>While Elo\u2019s design inherently assumes transitivity, our synthetic data, which are derived from\nrealistic scenarios, uncovers certain circumstances that violate this assumption. Such anomalies can\nsubsequently affect the final ranking of language models and their relative performance assessments.\n<br><br>3.2\nAxiom 2: Reliability\n<br><br>We consider two aspects of reliability:\n<br><br>Sensitivity to ordering: Unlike chess or time-bound sports where match sequences are structured,\nin LLM evaluations all matches can occur independently and in parallel, amplifying the sequence\u2019s\ninfluence on final models ranking. If the prompts are presented in a specific order, and one model\n<br><br>4\n<br><br>"
        },
        {
          "figures": [
            {
              "ref": "Figure 2",
              "url": "/static/figures/25523e27-7460-4513-a5a3-6c207a58fd65.svg"
            }
          ],
          "page_number": 5,
          "text": "<a href='#' class='figure-link' data-figure-url='/static/figures/25523e27-7460-4513-a5a3-6c207a58fd65.svg'>Figure 2</a>: Variation of Model A\u2019s average Elo score with increasing number of permutations (Nperms)\nfor different probabilities of Model A winning (Prob(A beats B)). Error bars indicate standard\nerrors of the mean.\n<br><br>happens to perform better on the initial set of prompts, it may gain an advantage in subsequent\ncomparisons due to the cumulative effect of its early success. This inherent variability prompts us\nto investigate the extent to which match-up ordering affects the robustness of Elo ratings.\n<br><br>Sensitivity to hyperparameters: The sensitivity of hyperparameters can compromise the ro-\nbustness of elo scores leading to inconsistent rankings. Evaluating and understanding this sensitivity\nis crucial for building evaluation frameworks that maintain consistency across diverse models. In\nthis work, we evaluate the sensitivity of Elo performance to one key hyperparameter, the K factor.\nThis factor acts as a scaling constant in the Elo rating system, pivotal for updating ratings after\neach matching. It essentially determines how quickly a model\u2019s rating converges to what can be\nconsidered its \u201ctrue\u201d skill. While conventional applications like chess use standard K-factor values\n(16 for experienced players and 32 for novices), these may not be directly applicable in the context\nof evaluating LLMs due to the unique characteristics and requirements of this domain.\n<br><br>4\nSynthetic Human Feedback\n<br><br>Given the costly and time-consuming nature of human evaluations, studying the Elo system\u2019s behav-\nior under various scenarios becomes challenging. To circumvent these limitations, we first validate\nproperties of Elo using synthetic data generation through Bernoulli processes to simulate various\nhuman feedback scenarios.\nIn 6 we will extend these evaluations to include real-world human\nfeedback. This time-agnostic and independent setup of LLM evaluations resembles a Bernoulli pro-\ncess (Bernoulli, 1713), a sequence of independent experiments, each yielding a simple \u201cwin\u201d or \u201closs\u201d\noutcome, representing one model outperforming another. We use this setting where we can control\ncharacteristics of the distribution to evaluate different desirable properties of a rating system.\n<br><br>In this controlled setting, our primary objectives include testing the transitivity axiom\u2014whether a\nconsistently higher-rated model outperforms those with lower ratings in all scenarios. Additionally,\nin studying the reliability axiom, we explore how the Elo scores are affected by the order in which\n<br><br>5\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 6,
          "text": "models are compared and the sensitivity to hyperparameter adjustments, particularly the K-factor.\nThis synthetic setup offers a robust platform to dissect and understand the dynamics of the Elo\nrating system in the context of LLM evaluations, without the constraints and limitations of relying\nsolely on real-world human feedback.\n<br><br>4.1\nThe Bernoulli Analogy\n<br><br>Pairwise comparisons in LLM evaluation draw parallels with the foundational principles of the\nBernoulli experiment in probability theory. This section delves into the similarity between human\nfeedback-based evaluations and the Bernoulli experiment\u2019s principles.\n<br><br>4.1.1\nPreliminaries\n<br><br>A Bernoulli trial is a random experiment with exactly two possible outcomes, \u201csuccess\u201d or \u201cfailure\u201d.\nThese outcomes adhere to the condition:\n<br><br>P(A) + P(Ac) = 1\n(3)\n<br><br>Here, the random variable X denotes the outcome, where X = 1 implies success, and X = 0 signifies\nfailure. The probabilities are:\n<br><br>P(X = 1)\n= p,\nP(X\n= 0) = 1 \u2212p\n(4)\n<br><br>with 0 \u2264p \u22641, the \u201csuccess\u201d probability.\n<br><br>4.1.2\nMapping to Human Feedback\n<br><br>When comparing two models, A and B, across N pairwise evaluations, the setup aligns with a\nBernoulli process. This process comprises a sequence of independent and identically distributed\n(i.i.d) Bernoulli trials. To frame this analogy, we designate a win probability, P(Awin), to model\nA. Leveraging a Bernoulli random variable, X, as a means to simulate synthetic human feedback,\nwe proceed as follows:\n<br><br>1. A sample is drawn from X using P(Awin).\n<br><br>2. If X = 1, feedback suggests a preference for model A.\n<br><br>3. Otherwise, model B is favored.\n<br><br>4.1.3\nExtending to Multiple Players\n<br><br>Given a finite set of n distinct models M, their pairwise comparisons can be formulated as:\n\u0012n\n2\n<br><br>\u0013\n=\nn!\n<br><br>2!(n \u22122)!\n(5)\n<br><br>This formula yields\n\u0000n\n2\n\u0001\nunique pairs (A, B) where A, B \u2208M and A \u0338= B. For each of these pairs, a\nBernoulli process, comprising multiple Bernoulli experiments, is conducted to discern which model\nperforms better over a sequence of trials.\n<br><br>6\n<br><br>"
        },
        {
          "figures": [
            {
              "ref": null,
              "url": "/static/figures/29b37aef-94b0-4514-aac2-71b9dd14c85c.png"
            },
            {
              "ref": "Figure 3",
              "url": "/static/figures/928dcdd3-fec4-4e10-a7a7-991120c339e6.png"
            }
          ],
          "page_number": 7,
          "text": "(a) Elo Scores for a Single Sequence\n(b) Elo Scores Averaged Over 100 Permutations\n<br><br><a href='#' class='figure-link' data-figure-url='/static/figures/928dcdd3-fec4-4e10-a7a7-991120c339e6.png'>Figure 3</a>: Final Elo scores difference (SA\u2212SB) as a function of K-factor and Nperms. Positive values\nreflect the expected ranking where Model A is superior to Model B, while negative values indicate\na discrepancy, falsely suggesting that Model B has a higher Elo score than Model A. We compare\nbetween a single sequence of outcomes and averages over Nperms = 100 unique permutations.\n<br><br>4.2\nSynthetic Data Generation\n<br><br>Building upon the Bernoulli process analogy, when conducting multiple independent evaluations\nbetween two models, the distribution of the number of times one model is preferred over the other\nnaturally follows a binomial distribution. For N pairwise comparisons, the relation is:\n<br><br>P(k; N, p) =\n\u0012N\nk\n<br><br>\u0013\npk(1 \u2212p)N\u2212k\n(6)\n<br><br>where P(k; N, p) is the probability of one model being preferred k times out of N evaluations. p\nis the success probability and\n\u0000N\nk\n\u0001\nis the binomial coefficient, representing the number of ways to\nchoose k successes from N trials.\n<br><br>5\nHow Robust Are Elo Scores?\n<br><br>This section defines rigorous stress tests designed to investigate the whether the two axioms are sat-\nisfied in this evaluation framework. We focus on critical desirable properties of a ranking mechanism\n\u2013 that it should 1) be insensitive to match-up ordering, 2) not be overly sensitive to hyperparameters\nlike K-factor 3) preserve properties of transitivity. Subsequently, we provide empirically-grounded\nguidelines for safe and interpretable application of Elo ratings.\n<br><br>5.1\nImpact of Ordering on Elo Ratings\n<br><br>5.1.1\nExperimental Setup\n<br><br>To quantify the effect of match-up ordering on Elo ratings, we generate a baseline sequence of\nNgames = 1000 match outcomes between models A and B, reflecting the scale typical of LLM\nevaluations via human feedback. We hold Ngames constant for the entirety of our study to maintain\n<br><br>7\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 8,
          "text": "consistency. From this baseline, we derive Nperms distinct permutations, each involving a complete\nreshuffling of the initial sequence to simulate various chronological orders in which the games might\nunfold. It is important to note that we are not generating new match outcomes for each permutation;\ninstead, we simply reorder the existing data to explore the potential impact of different match-up\nsequences. For each reordered sequence, we update the Elo ratings RA and RB according to equation\n2, resetting both ratings to an initial value of 1400 at the start of each permutation. Following this,\nwe compute average Elo ratings per match across all Nperms permutations, ensuring a robust analysis\nthat takes into account the full range of possible match-up orders.\n<br><br>We compare ratings\u2019 behavior for a set of selected winning probabilities Prob(A beats B) = {0.51,\n0.55, 0.6}, inspecting a spectrum of real-world scenarios. Nperm is varied from a minimum of 1 to a\nmaximum of 10k, providing a robust sample size for statistical analysis (see Figure 2). Subsequently,\nwe compute the average Elo ratings per match across all permutations. These averages, \u00afRA and\n\u00afRB. particularly for Nperms = 1 and Nperm = 100, are visualized to offer insights into the stability\nof the ratings, as shown in Figure 1.\n<br><br>5.1.2\nKey Findings\n<br><br>Our analysis underscores the interplay between winning probability P(Awin) and the number of\ndifferent orderings Nperm on the stability of Elo ratings after each update. For P(Awin) \u22650.6, Elo\nratings demonstrate high stability; additional results for P(Awin) = 0.65 and beyond are available\nin Appendix B. On the other hand, for P(Awin) \u22480.5, ratings exhibit significant instability for a\nsingle sequence. As depicted in Figure 1, when both models have a win probabilities are around 0.5,\nElo ratings frequently intertwine, making it challenging to discern a clear performance difference\nbetween the two. The instability plateaus as Nperms exceeds 100, resulting in stabilized Elo ratings\nthat align closely with the preset winning probabilities.\nFor instance, at P(Awin) = 0.55, the\naverage Elo rating for Model A, \u00afRA, consistently exceeds that for Model B, \u00afRB, when averaged\nacross multiple permutations, reflecting an accurate performance-based ranking of these models.\n<br><br>These observations validate our concerns highlighted earlier, emphasizing the critical role of Nperms\nfor a reliable interpretation of Elo ratings in LLM evaluations. In Elo-based evaluations, the sequence\nof which models are compared is not a mere procedural detail; it can significantly influence the final\nElo scores. In scenarios involving models of similar quality and capabilities, which is often the case,\nthis sensitivity is exacerbated.\n<br><br>5.2\nSensitivity to Hyperparameters\n<br><br>5.2.1\nExperimental Setup\n<br><br>We extend our previous approach by conducting tests across a range of winning probabilities and\nmultiple K-factor values (1, 8, 16, 32, 64). We compute and compare the average Elo scores \u00afSA and\n\u00afSB for Ngames = 1000 and Nperms = {1, 100}. The differences between these final averages for Model\nA and Model B are summarized in Figure 3 to assess the stability and expected ranking between\nthe two models.\n<br><br>8\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 9,
          "text": "Table 1: Investigation of Elo score reliability in capturing true model hierarchies across varying\nconfigurations. Scenarios explore the transitive relationship A > B and B > C\n=\u21d2\nA > C.\nThe star (*) indicates cases where the Elo score fails to accurately reflect the expected hierarchy\nof models. \u2248represents models with similar performance; \u226bindicates that a model significantly\noutperforms the other one.\n<br><br>Scenario\nModel\nModels Ranking per Configuration\n<br><br>N = 1, K = 1\nN = 100, K = 1\nN = 1, K = 16\nN = 100, K = 16\n<br><br>K\nA \u226bB\nB \u226bC\n<br><br>A\n1539.43\n1528.50 \u00b1 0.35\n1650.93\n1584.78 \u00b1 3.09\n<br><br>B\n1390.47\n1410.33 \u00b1 0.54\n1381.17\n1406.48 \u00b1 3.23\n<br><br>C\n1270.10\n1261.17 \u00b1 0.33\n1167.90\n1208.74 \u00b1 2.71\n<br><br>R\nA \u226bB\nB \u2248C\n<br><br>A\n1502.09\n1495.92 \u00b1 0.36\n1509.08\n1526.04 \u00b1 3.03\n<br><br>B\n1337.48\n1342.70* \u00b1 0.53\n1379.00\n1340.83 \u00b1 2.83\n<br><br>C\n1360.42\n1361.38* \u00b1 0.38\n1311.92\n1333.13 \u00b1 2.68\n<br><br>B\nA \u2248B\nB \u226bC\n<br><br>A\n1437.97\n1433.84* \u00b1 0.41\n1440.31\n1460.22 \u00b1 2.90\n<br><br>B\n1455.10\n1453.84* \u00b1 0.61\n1481.04\n1452.87 \u00b1 3.25\n<br><br>C\n1306.93\n1312.32 \u00b1 0.34\n1278.65\n1286.91 \u00b1 2.72\n<br><br>N\nA \u2248B\nB \u2248C\n<br><br>A\n1426.33\n1419.73 \u00b1 0.36\n1407.44\n1432.26 \u00b1 2.93\n<br><br>B\n1390.47\n1393.29 \u00b1 0.59\n1386.17\n1392.75 \u00b1 3.04\n<br><br>C\n1383.20\n1386.99 \u00b1 0.41\n1406.39\n1374.99 \u00b1 3.12\n<br><br>5.2.2\nKey Findings\n<br><br>As shown in Figure 3, notable instability is observed in model rankings based on the final Elo scores\nwhen we consider a single sequence of paired comparisons (i.e., Nperms = 1), especially for winning\nprobabilities nearing 0.5. This instability is markedly exacerbated at higher K-factors. In contrast,\nthe picture changes when coupling higher K-factors with raising the number of permutations to at\nleast 100.\n<br><br>Higher K-factors, in this multi-permutation scenario, speed up the differentiation between models\u2019\nElo scores, enabling faster convergence to their true skill levels. This yields much more stable and\nreliable model rankings. It is noteworthy that this faster convergence is observed to be more reliable\nfor higher winning probabilities, which corresponds to skewed win rates in a real-wold scenario.\n<br><br>5.3\nTransitive Properties of Elo Scores\n<br><br>5.3.1\nExperimental Setup\n<br><br>The transitivity property of the Elo scores is defined as:\n<br><br>A > B\nand\nB > C =\u21d2A > C\n(7)\n<br><br>To test the transitivity property, we design four distinct scenarios that model real-world conditions:\n<br><br>K\nModel A beats model B and model B beats model C both with high win probabilities (Pwin =\n0.75).\n<br><br>R\nModel A beats model B with a high win probability (Pwin = 0.75), model B beats model C\nwith a win probability close to 0.5 (Pwin = 0.51).\n<br><br>9\n<br><br>"
        },
        {
          "figures": [
            {
              "ref": "Figure 4",
              "url": "/static/figures/8b88d855-1984-4c8b-8f62-5432a3a67571.png"
            },
            {
              "ref": null,
              "url": "/static/figures/68f6455d-ac59-4c5e-ae2b-ad3b56868f89.png"
            }
          ],
          "page_number": 10,
          "text": "(a) Experiment: Flan-t5-xxl vs. Flan-t5-xl\nRecorded Win rates: 0.64 vs 0.36\n<br><br>(b) Experiment: Dolly-v2-7b vs. Dolly-v2-12b\nRecorded Win rates: 0.51 vs 0.49\n<br><br><a href='#' class='figure-link' data-figure-url='/static/figures/8b88d855-1984-4c8b-8f62-5432a3a67571.png'>Figure 4</a>: Final Elo scores difference (SA \u2212SB) as a function of K-factor and Nperms. This figure\ncompares Model A (Flan-t5-xxl) and Model B (Flan-t5-xl). Positive values indicate the expected\nranking where Model A is superior to Model B, while negative values indicate a discrepancy,\nfalsely suggesting that Model B has a higher Elo score than Model A.\n<br><br>B\nModel A beats model B with a win probability close to 0.5 (Pwin = 0.51), model B beats\nmodelC with a high win probability (Pwin = 0.75).\n<br><br>N\nModel A beats model B with a win probability of 0.54, model B beats model C with a win\nprobability of 0.51.\n<br><br>In each of these scenarios, we simulate matches for paired comparisons \"A vs. B\" and \"B vs.\nC\" and then rearrange these matches in an arbitrary order to form our baseline sequence. This\napproach mimics how Elo ratings are computed for online leaderboards in the evaluation of large\nlanguage models (Wu et al., 2023; Lin & Chen, 2023). We then analyze whether Elo scores maintain\nthe expected model hierarchies.\n<br><br>5.3.2\nKey Findings\n<br><br>The results of all 4 scenarios are consolidated in Table 1. These outcomes validate that the transi-\ntivity assumed by the Elo rating system can be vulnerable, especially when win rates hover around\n\u224850%. Once again, we observe that varying the number of permutations (n = 1 vs Nperms = 100)\nand the K-factor plays a critical role in stability. In the R and B scenarios, with Nperms = 100 and\nK = 1, we notice discrepancies in the models\u2019 rankings. This can be contrasted with K = 16, where\nrankings were much more consistent and reliable. The slower updates from K = 1 suggest that\nthis setting is possibly too conservative to capture the transitive relations quickly, hence leading to\ninconsistencies.\n<br><br>6\nValidation on Real-World Human Feedback\n<br><br>Building on the insights gained from our synthetic data experiments, we extend our validation\nefforts to include real-world human feedback. Our objective is two-fold: first, to ascertain how the\n<br><br>10\n<br><br>"
        },
        {
          "figures": [
            {
              "ref": "Figure 5",
              "url": "/static/figures/bc98cc7a-21a4-4f88-8f83-8672d86cdea6.png"
            },
            {
              "ref": null,
              "url": "/static/figures/49189e8e-bcc1-4b7c-a924-a1710f69e89c.png"
            }
          ],
          "page_number": 11,
          "text": "(a) Flan-t5-xxl vs. Flan-t5-xl and Flan-t5-xxl vs.\nDolly-v2-12b\nRecorded Win rates: 0.64 vs 0.36 and\n0.79 vs 0.21\n<br><br>(b) Dolly-v2-7b vs. Dolly-v2-12b and Flan-t5-xxl\nvs. Dolly-v2-12b\nRecorded Win rates: 0.51 vs 0.49 and\n0.79 vs 0.21\n<br><br><a href='#' class='figure-link' data-figure-url='/static/figures/bc98cc7a-21a4-4f88-8f83-8672d86cdea6.png'>Figure 5</a>: Final Elo scores (SA, SB and SC) for three different models at multiple configurations of\nNperms = {1, 100} and K-factor = {1, 8, 16, 32}. Intersecting of surfaces of individual model scores\nsignifies that the relative ranking of the models is sensitive to these configurations. The order of\nmodel overlaps represent these models ranking based on their Elo scores.\n<br><br>Table 2: Win rates per evaluated model across conducted paired comparison experiments.\n<br><br>Experiment\nWin Rate\n<br><br>Flan-t5-xxl\n0.79\nDolly-v2-12b\n0.21\n<br><br>Flan-t5-xxl\n0.64\nFlan-t5-xl\n0.36\n<br><br>Dolly-v2-7b\n0.51\nDolly-v2-12b\n0.49\n<br><br>demonstrated properties established using synthetic data generalize to real human annotations; and\nsecond, to evaluate the Elo rating system\u2019s utility for assessing large language models (LLMs) under\npractical conditions.\n<br><br>6.1\nExperimental Setup\n<br><br>Our study leverages human feedback data previously collected to explore data prioritization in lan-\nguage model evaluations. For details about our pool of prompts and models, completion generation,\nand annotation collection process, we refer the reader to the experimental setup section of our previ-\nous work (Boubdir et al., 2023). We focus on models from the well-established Dolly (Conover et al.,\n2023) and Flan (Chung et al., 2022) families, ensuring relevance to the broader NLP community.\n<br><br>11\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 12,
          "text": "The evaluation dataset consists of 400 prompts, with 100 randomly chosen from the SODA (Kim\net al., 2022) dataset and 100 from each of the CommonsenseQA (Talmor et al., 2019), Common-\nGen (Lin et al., 2020), and AdversarialQA (Bartolo et al., 2020) subsets, all of which are part of\nthe Public Pool of Prompts (P3) dataset (Sanh et al., 2021). This ensures a diverse set of evaluation\nscenarios for a comprehensive assessment of the models\u2019 capabilities. Consistent with our synthetic\ndata methodology, tie outcomes have been excluded from this analysis to focus specifically on the\nimplications for the robustness of Elo scores.\n<br><br>In line with our previous analyses, we continue to explore the influence of variations in Nperms =\n{1, 100} and the K-factor (ranging from 1 to 36) on the robustness and reliability of Elo scores.\nThe win rates for each model, derived from human evaluations, are summarized in Table 2. Our\nreal-world experiments yield two distinct types of scenarios: i) one in which a model decisively\noutperforms the other, such as the Flan-t5-xxl vs. Flan-t5-xl pairing; and ii) another one with two\nmodels nearly evenly matched, as in the Dolly-v2-7b vs. Dolly-v2-12b case.\n<br><br>6.2\nKey Findings\n<br><br>Our analysis of real-world human feedback data reveals that the stability of Elo ratings is influenced\nby the disparities in win rates and the choice of hyperparameters K-factor and Nperms. In situations\nwhere win rates show a significant discrepancy, such as in our Flan family experiment, Elo ratings\nremain notably consistent across different K-factors and Nperms configurations (see Figure 7). On\nthe other hand, in cases like the Dolly family experiment where win rates are closely matched, the\nElo rating system exhibits higher volatility at Nperms = 1 but gains stability at Nperms = 100 at\nrelatively small K-factors (see Figure 4b).\n<br><br>Regarding the conservation of transitivity, our findings indicate that this property is not universally\nmaintained in real-world human evaluations, as observed in synthetic data in Section 5. The relative\nrankings of models that perform similarly are sensitive to the choice of hyperparameters K-factor\nand Nperms. Consequently, one should exercise caution in drawing conclusions from the Elo scores\nwhen comprehensive paired comparison data, as dictated by the combination formula 5, is not\navailable. Our observations are in line with the trends seen in our synthetic data experiments.\n<br><br>7\nEmpirical Guidelines for Robust Elo-based Evaluation\n<br><br>In this section, we distill essential practices for enhancing the reliability of Elo-based evaluation of\nlanguage models. These guidelines, derived from our empirical findings, differ notably from some\nconventional Elo settings and have significant implications for current real-world applications:\n<br><br>\u2022 Achieving Score Stability: To obtain stable and reliable Elo ratings, it\u2019s recommended to\nrun numerous permutations, ideally with Nperm \u2265100. This approach significantly improves\nthe consistency of outcomes over single or fewer permutations commonly used.\n<br><br>\u2022 Adjusting the K-factor: A smaller K-factor may reduce significant rating fluctuations when\nmodels have closely matched win rates.\n<br><br>\u2022 Rapid Convergence for Clear Winners: When there\u2019s a clear performance disparity\nbetween models, a higher K-factor accelerates the alignment of Elo ratings with the models\u2019\n<br><br>12\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 13,
          "text": "\u201ctrue\u201d performance levels. This is in stark contrast to traditional uses of Elo ratings, where a\none-size-fits-all K-factor is frequently applied.\n<br><br>\u2022 Transitivity is not guaranteed: The assumption that (A beats B and B beats C implies\nA > C) is not consistently valid in Elo ratings.This is particularly invalid when models have\nsimilar performance levels, challenging a common assumption in many Elo-based evaluations.\n<br><br>These guidelines serve as empirically-grounded recommendations to improve the robustness and\ninterpretability of Elo-based evaluations for LLMs.\nFollowing these best practices will help in\nyielding more reliable conclusions on models\u2019 performance via human judgment.\n<br><br>8\nRelated Work\n<br><br>Several works have proposed improvements to Elo scores. Variants such as Glicko (Glickman, 1995;\n1999; 2012) and TrueSkill\u2122(Herbrich et al., 2006; Minka et al., 2018) have incorporated more\ncomplex statistical methods into the original Elo framework, to address some of the limitations of\nthe Elo rating system, particularly in the context of games with more than two players or teams,\nor games with more complex outcomes than just win or loss. There is also ongoing research into\nthe efficacy of these systems in diverse and dynamic environments (Dehpanah et al., 2021; Bertrand\net al., 2023).\nPrior work has demonstrated some limitations of Elo in maintaining transitivity,\nespecially in non-transitive cyclic games such as rock-paper-scissors and StarCraft ii\n(Bertrand\net al., 2023; Vadori & Savani, 2023). However, our work diverges by focusing on the reliability of\nElo applied to large language model systems. To-date there has not been a comprehensive evaluation\nin this context.\n<br><br>Independent from Elo, numerous studies have explored how sensitivity to hyperparameters can\nundermine the generalization of findings (Novak et al., 2018; Lucic et al., 2018; Henderson et al.,\n2017; Kadlec et al., 2017; Bouthillier et al., 2021) in machine learning. This forms part of a wider\nbody of work which considers which factors influence reliability and reproducibility (Goodman et al.,\n2016; Gundersen & Kjensmo, 2018; Barba, 2018; Drummond, 2009). Notable directions includes\nstudies on the impact of random seeds (Nagarajan et al., 2018; Madhyastha & Jain, 2019; Summers\n& Dinneen, 2021), model design choices (Shamir et al., 2020; Snapp & Shamir, 2021; Pozzobon\net al., 2023; Ko et al., 2023), the use of data parallelism (Shallue et al., 2019), hardware (Zhuang\net al., 2021) and test set construction (S\u00f8gaard et al., 2021; Lazaridou et al., 2021; Melis et al.,\n2018). Our work is complementary to these efforts, providing a rigorous evaluation of the impact\nof key hyperparameters and experimental settings on Elo performance.\n<br><br>9\nConclusion\n<br><br>This paper provides a comprehensive study on the reliability of the Elo rating system for eval-\nuating LLMs using human feedback using an axiomatic framework. We identify various factors\nthat influence the robustness of Elo ratings and offer guidelines for their effective application in\nreal-world scenarios. While our findings lay down an essential framework, they are by no means\nexhaustive. Future work could extend the present study by considering tie outcomes and adopting\nmulti-category Bernoulli synthetic data to more closely simulate the varied landscape of human\nfeedback. Such extensions could provide additional insights into the convergence properties of the\n<br><br>13\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 14,
          "text": "Elo rating system in the fast-evolving landscape of language models.\n<br><br>References\n<br><br>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,\nNicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernan-\ndez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark,\nSam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory\nfor alignment, 2021.\n<br><br>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson\nKernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez,\nTristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson,\nDario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Ka-\nplan. Training a helpful and harmless assistant with reinforcement learning from human feedback,\n2022a.\n<br><br>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson,\nChristopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson,\nEthan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile\nLukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova\nDasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El\nShowk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan,\nTristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas\nJoseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from\nai feedback, 2022b.\n<br><br>Lorena A. Barba. Terminologies for reproducible research, 2018.\n<br><br>Max Bartolo, A Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat the ai:\nInvestigating adversarial human annotation for reading comprehension. Transactions of the As-\nsociation for Computational Linguistics, 8:662\u2013678, 2020. URL https://api.semanticschola\nr.org/CorpusID:211010520.\n<br><br>Jakob Bernoulli. Ars conjectandi, opus posthumum. Accedit Tractatus de seriebus infinitis, et epistola\ngallic\u00e9 scripta de ludo pilae reticularis. Thurneysen Brothers, Basel, 1713.\n<br><br>Quentin Bertrand, Wojciech Marian Czarnecki, and Gauthier Gidel. On the limitations of the elo,\nreal-world games are transitive, not additive. In Francisco Ruiz, Jennifer Dy, and Jan-Willem\nvan de Meent (eds.), Proceedings of The 26th International Conference on Artificial Intelligence\nand Statistics, volume 206 of Proceedings of Machine Learning Research, pp. 2905\u20132921. PMLR,\n25\u201327 Apr 2023. URL https://proceedings.mlr.press/v206/bertrand23a.html.\n<br><br>John J. Binder and Murray Findlay. The effects of the bosman ruling on national and club teams\nin europe. Journal of Sports Economics, 13:107\u2013129, 2009. URL https://api.semanticschola\nr.org/CorpusID:153873146.\n<br><br>Meriem Boubdir, Edward Kim, Beyza Ermis, Marzieh Fadaee, and Sara Hooker. Which prompts\nmake the difference? data prioritization for efficient human llm evaluation, 2023.\n<br><br>14\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 15,
          "text": "Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin\nSzeto, Nazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti, Samira\nEbrahimi Kahou, Vincent Michalski, Tal Arbel, Chris Pal, Gael Varoquaux, and Pascal Vincent.\nAccounting for variance in machine learning benchmarks. In A. Smola, A. Dimakis, and I. Stoica\n(eds.), Proceedings of Machine Learning and Systems, volume 3, pp. 747\u2013769, 2021. URL https://\nproceedings.mlsys.org/paper/2021/file/cfecdb276f634854f3ef915e2e980c31-Paper.pdf.\n<br><br>Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan\nYi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang,\nand Xing Xie. A survey on evaluation of large language models, 2023.\n<br><br>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu,\nVincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean,\nJacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-\nfinetuned language models, 2022. URL https://arxiv.org/abs/2210.11416.\n<br><br>Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick\nWendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world\u2019s first truly open\ninstruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/04/12/dolly-fir\nst-open-commercially-viable-instruction-tuned-llm.\n<br><br>Arman Dehpanah, Muheeb Faizan Ghori, Jonathan F. Gemmell, and Bamshad Mobasher. Evaluat-\ning team skill aggregation in online competitive games. 2021 IEEE Conference on Games (CoG),\npp. 01\u201308, 2021. URL https://api.semanticscholar.org/CorpusID:235593176.\n<br><br>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314, 2023.\n<br><br>Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong\nSun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional\nconversations, 2023.\n<br><br>Chris Drummond. Replicability is not reproducibility: Nor is it good science. In Evaluation Meth-\nods for Machine Learning Workshop at the 26th International Conference on Machine Learning,\nICML, 2009.\n<br><br>Aram Ebtekar and Paul Liu. Elo-mmr: A rating system for massive multiplayer competitions. In\nProceedings of the Web Conference 2021, WWW \u201921, pp. 1772\u20131784, New York, NY, USA, 2021.\nAssociation for Computing Machinery. ISBN 9781450383127. doi: 10.1145/3442381.3450091.\nURL https://doi.org/10.1145/3442381.3450091.\n<br><br>Arpad E. Elo. The Rating of Chessplayers, Past and Present. Arco Pub., New York, 1978. ISBN\n0668047216 9780668047210. URL http://www.amazon.com/Rating-Chess-Players-Past-Pre\nsent/dp/0668047216.\n<br><br>ESL. Ranking - dota2 - esl pro tour. URL https://pro.eslgaming.com/tour/dota2/ranking/.\n<br><br>Mark E Glickman. A comprehensive guide to chess ratings. American Chess Journal, pp. 59\u2013102,\n1995.\n<br><br>15\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 16,
          "text": "Mark E Glickman. Parameter estimation in large dynamic paired comparison experiments. Applied\nStatistics, pp. 377\u2013394, 1999.\n<br><br>Mark E Glickman. Example of the glicko-2 system. Boston University, pp. 1\u20136, 2012.\n<br><br>Steven N. Goodman, Daniele Fanelli, and John P. A. Ioannidis. What does research reproducibility\nmean?\nScience Translational Medicine, 8(341):341ps12\u2013341ps12, 2016. ISSN 1946-6234. doi:\n10.1126/scitranslmed.aaf5027.\n<br><br>Odd Erik Gundersen and Sigbj\u00f8rn Kjensmo. State of the art: Reproducibility in artificial intelli-\ngence. In AAAI, 2018.\n<br><br>Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.\nDeep reinforcement learning that matters. CoRR, abs/1709.06560, 2017.\n<br><br>Ralf Herbrich, Tom Minka, and Thore Graepel. Trueskill\u2122: A bayesian skill rating system. In\nB. Sch\u00f6lkopf, J. Platt, and T. Hoffman (eds.), Advances in Neural Information Processing Systems,\nvolume 19. MIT Press, 2006. URL https://proceedings.neurips.cc/paper_files/paper/2\n006/file/f44ee263952e65b3610b8ba51229d1f9-Paper.pdf.\n<br><br>Lars Magnus Hvattum and Halvard Arntzen.\nUsing elo ratings for match result prediction in\nassociation football. International Journal of Forecasting, 26(3):460\u2013470, 2010. URL https:\n//EconPapers.repec.org/RePEc:eee:intfor:v:26:y::i:3:p:460-470.\n<br><br>Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert\nMcHardy. Challenges and applications of large language models, 2023.\n<br><br>Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst. Knowledge base completion: Baselines strike\nback.\nIn Proceedings of the 2nd Workshop on Representation Learning for NLP, pp. 69\u201374,\nVancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/\nW17-2609.\n<br><br>Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le\nBras, Malihe Alikhani, Gunhee Kim, Maarten Sap, and Yejin Choi. Soda: Million-scale dialogue\ndistillation with social commonsense contextualization. ArXiv, abs/2212.10465, 2022.\n<br><br>Wei-Yin Ko, Daniel D\u2019souza, Karina Nguyen, Randall Balestriero, and Sara Hooker. Fair-ensemble:\nWhen fairness naturally emerges from deep ensembling, 2023.\n<br><br>Andreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00e1rd Nagyfi, Shahul ES, Sameer Suri,\nDavid Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and\nAlexander Mattick. Openassistant conversations \u2013 democratizing large language model alignment,\n2023.\n<br><br>Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d\u2019Autume, Sebastian Ruder, Dani Yogatama, Kris Cao,\nTomas Kocisky, Susannah Young, and Phil Blunsom. Pitfalls of static language modelling, 2021.\n<br><br>Christoph Leitner, Achim Zeileis, and Kurt Hornik.\nForecasting sports tournaments by ratings\nof (prob)abilities: A comparison for the euro 2008. International Journal of Forecasting, 26(3):\n471\u2013481, 2010. ISSN 0169-2070. doi: https://doi.org/10.1016/j.ijforecast.2009.10.001. URL\n<br><br>16\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 17,
          "text": "https://www.sciencedirect.com/science/article/pii/S0169207009001459.\nSports\nForecasting.\n<br><br>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,\nYian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang\nYuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R\u00e9, Di-\nana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong,\nHongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuk-\nsekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Hen-\nderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan\nMai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models, 2022.\n<br><br>Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and\nXiang Ren. CommonGen: A constrained text generation challenge for generative commonsense\nreasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1823\u2013\n1840, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/20\n20.findings-emnlp.165. URL https://aclanthology.org/2020.findings-emnlp.165.\n<br><br>Yen-Ting Lin and Yun-Nung Chen. LLM-Eval: Unified multi-dimensional automatic evaluation for\nopen-domain conversations with large language models, 2023.\n<br><br>Liquipedia. Elo rating - liquipedia starcraft brood war wiki. URL https://liquipedia.net/sta\nrcraft/Elo_rating.\n<br><br>Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are gans created\nequal? a large-scale study, 2018.\n<br><br>Pranava Madhyastha and Rishabh Jain.\nOn model stability as a function of random seed.\nIn\nProceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pp.\n929\u2013939, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:\n10.18653/v1/K19-1087. URL https://aclanthology.org/K19-1087.\n<br><br>G\u00e1bor Melis, Chris Dyer, and P. Blunsom. On the state of the art of evaluation in neural language\nmodels. ArXiv, abs/1707.05589, 2018.\n<br><br>Tom Minka, Ryan Cleven, and Yordan Zaykov. Trueskill 2: An improved bayesian skill rating\nsystem. Technical Report MSR-TR-2018-8, Microsoft, March 2018. URL https://www.micros\noft.com/en-us/research/publication/trueskill-2-improved-bayesian-skill-rating-s\nystem/.\n<br><br>Prabhat Nagarajan, Garrett Warnell, and Peter Stone. The impact of nondeterminism on repro-\nducibility in deep reinforcement learning. In 2nd Reproducibility in Machine Learning Workshop\nat ICML, July 2018.\n<br><br>Roman Novak, Yasaman Bahri, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein.\nSensitivity and generalization in neural networks: an empirical study. In International Conference\non Learning Representations, 2018. URL https://openreview.net/forum?id=HJC2SzZCW.\n<br><br>Luiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara Hooker. On the challenges of using black-box\napis for toxicity evaluation in research, 2023.\n<br><br>17\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 18,
          "text": "April M. Reid. Elo rating system for video games explained. URL https://esportsheadlines.c\nom/elo-rating-system-for-video-games-explained/.\n<br><br>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, An-\ntoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen\nXu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani,\nNihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Man-\nica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala\nNeeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask\nprompted training enables zero-shot task generalization, 2021.\n<br><br>Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and\nGeorge E. Dahl. Measuring the effects of data parallelism on neural network training, 2019.\n<br><br>Gil I. Shamir, Dong Lin, and Lorenzo Coviello. Smooth activations and reproducibility in deep\nnetworks, 2020.\n<br><br>Robert R. Snapp and Gil I. Shamir.\nSynthesizing irreproducibility in deep networks.\nCoRR,\nabs/2102.10696, 2021.\n<br><br>Anders S\u00f8gaard, Sebastian Ebert, Jasmijn Bastings, and Katja Filippova. We need to talk about\nrandom splits. In Proceedings of the 16th Conference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, pp. 1823\u20131832, Online, April 2021. Association for\nComputational Linguistics. doi: 10.18653/v1/2021.eacl-main.156. URL https://aclanthology\n.org/2021.eacl-main.156.\n<br><br>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Ko-\ncurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda\nAskell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Johan\nAndreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlm\u00fcller, Andrew M. Dai, Andrew\nLa, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta,\nAnna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul\nMenezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat,\nAykut Erdem, Ayla Karaka\u015f, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bart\u0142omiej Bo-\njanowski, Batuhan \u00d6zyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno\nStein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron\nDour, Catherine Stinson, Cedrick Argueta, Cesar Ferri, Chandan Singh, Charles Rathkopf, Chen-\nlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Christopher Waites, Christian Voigt,\nChristopher D Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro,\nColin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks,\nDan Kilman, Dan Roth, C. Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegu\u00ed\nGonz\u00e1lez, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David\nDohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis\nKleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan,\nDimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Do-\ngus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick,\nEmanuele Rodol\u00e0, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A\n<br><br>18\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 19,
          "text": "Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii,\nFanyue Xia, Fatemeh Siar, Fernando Mart\u00ednez-Plumed, Francesca Happ\u00e9, Francois Chollet, Frieda\nRong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germ\u00e1n Kruszewski, Giambattista\nParascandolo, Giorgio Mariani, Gloria Xinyue Wang, Gonzalo Jaimovitch-Lopez, Gregor Betz,\nGuy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh\nMehta, Hayden Bogar, Henry Francis Anthony Shevlin, Hinrich Schuetze, Hiromu Yakura, Hong-\nming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson\nKernion, Jacob Hilton, Jaehoon Lee, Jaime Fern\u00e1ndez Fisac, James B Simon, James Koppel,\nJames Zheng, James Zou, Jan Kocon, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema\nRadom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova,\nJelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng\nXu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan\nBatchelder, Jonathan Berant, J\u00f6rg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boude-\nman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil\nKanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Mark-\nert, Kaustubh Dhole, Kevin Gimpel, Kevin Omondi, Kory Wallace Mathewson, Kristen Chiafullo,\nKsenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca\nMoschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros-Col\u00f3n, Luke Metz,\nL\u00fctfi Kerem Senel, Maarten Bosma, Maarten Sap, Maartje Ter Hoeve, Maheen Farooqi, Manaal\nFaruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramirez-\nQuintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L Leavitt,\nMatthias Hagen, M\u00e1ty\u00e1s Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McEl-\nrath, Michael Andrew Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt,\nMichael Strube, Micha\u0142 Sw\u0119drowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike\nCain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor\nGeva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan Andrew Chi, Nayeon Lee, Neta\nGur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita\nNangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Con-\nstant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain\nEvans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol,\nPegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter W Chang, Peter Eckersley, Phu Mon\nHtut, Pinyu Hwang, Piotr Mi\u0142kowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei,\nQing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker,\nRamon Risco, Rapha\u00ebl Milli\u00e8re, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa,\nRobbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Le Bras,\nRosanne Liu, Rowan Jacobs, Rui Zhang, Russ Salakhutdinov, Ryan Andrew Chi, Seungjae Ryan\nLee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand,\nSam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel Stern\nSchoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean\nCasey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Ham-\ndan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane\nGu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima Shammie Debnath,\nSiamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan\nLee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella\nBiderman, Stephanie Lin, Stephen Prasad, Steven Piantadosi, Stuart Shieber, Summer Mish-\nerghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq\nAli, Tatsunori Hashimoto, Te-Lin Wu, Th\u00e9o Desbordes, Theodore Rothschild, Thomas Phan,\n<br><br>19\n<br><br>"
        },
        {
          "figures": [],
          "page_number": 20,
          "text": "Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gersten-\nberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra,\nVera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Venkatesh Ramasesh, vinay uday prabhu,\nVishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout\nVossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh,\nYair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen,\nYonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang,\nZijie J. Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: Quantifying and extrap-\nolating the capabilities of language models. Transactions on Machine Learning Research, 2023.\nISSN 2835-8856. URL https://openreview.net/forum?id=uyTL5Bvosj.\n<br><br>Cecilia Summers and Michael J. Dinneen. Nondeterminism and instability in neural network opti-\nmization. In International Conference on Machine Learning, 2021. URL https://api.semant\nicscholar.org/CorpusID:232146616.\n<br><br>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Papers), pp. 4149\u20134158, Minneapolis, Min-\nnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL\nhttps://aclanthology.org/N19-1421.\n<br><br>Nelson Vadori and Rahul Savani. Ordinal potential-based player rating, 2023.\n<br><br>Ben P. Wise. Elo ratings for large tournaments of software agents in asymmetric games. ArXiv,\nabs/2105.00839, 2021. URL https://api.semanticscholar.org/CorpusID:233481682.\n<br><br>Yuxiang Wu, Zhengyao Jiang, Akbir Khan, Yao Fu, Laura Ruis, Edward Grefenstette, and Tim\nRockt\u00e4schel. Chatarena: Multi-agent language game environments for large language models.\nhttps://github.com/chatarena/chatarena, 2023.\n<br><br>Yining Ye, Xin Cong, Yujia Qin, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Large language model\nas autonomous decision maker, 2023.\n<br><br>Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. Slic-hf:\nSequence likelihood calibration with human feedback, 2023.\n<br><br>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n<br><br>Donglin Zhuang, Xingyao Zhang, Shuaiwen Leon Song, and Sara Hooker. Randomness in neural\nnetwork training: Characterizing the impact of tooling.\nArXiv, abs/2106.11872, 2021.\nURL\nhttps://api.semanticscholar.org/CorpusID:235593009.\n<br><br>20\n<br><br>"
        },
        {
          "figures": [
            {
              "ref": "Figure 6",
              "url": "/static/figures/a3c4b4d6-5f25-44db-a12e-4f7fad06e950.svg"
            }
          ],
          "page_number": 21,
          "text": "A\nExtension to Multiple Outcomes\n<br><br>For scenarios where outcomes can extend beyond wins and losses, such as a tie option, the multi-\nnomial distribution becomes relevant. For the outcomes win, loss, and tie, we sample according to\nthe distribution:\n<br><br>P (nwin, nloss, ntie; N, pwin, ploss, ptie)\n<br><br>=\nN!\n<br><br>nwin!nloss!ntie!pnwin\nwin pnloss\nloss pntie\ntie\n(8)\n<br><br>B\nImpact of Ordering on Elo Ratings: Skewed Win Rates\n<br><br>We summarize our findings on the impact of match sequences on Elo ratings for winning probabilities\nProb(A beats B) \u22650.65.\n<br><br><a href='#' class='figure-link' data-figure-url='/static/figures/a3c4b4d6-5f25-44db-a12e-4f7fad06e950.svg'>Figure 6</a>: Impact of win probabilities and permutation sampling on Elo ratings: Com-\nparing Model A and Model B across three different win probabilities (Prob(A beats B) =\n0.9, 0.8, 0.7, 0.65) with two levels of permutation sampling (Nperms = 1 and Nperms = 100). The top\nrow displays the observed win rates, the middle row illustrates Elo ratings with a single permuta-\ntion, and the bottom row shows the mean and standard error of the mean (SEM) of Elo ratings\nacross 100 permutations.\n<br><br>21\n<br><br>"
        },
        {
          "figures": [
            {
              "ref": "Figure 7",
              "url": "/static/figures/f36faf98-e4d6-490b-8d1a-debc4cf4d3c3.svg"
            }
          ],
          "page_number": 22,
          "text": "C\nExperiment Flan-t5-xxl vs. Dolly-v2-12b Results\n<br><br><a href='#' class='figure-link' data-figure-url='/static/figures/f36faf98-e4d6-490b-8d1a-debc4cf4d3c3.svg'>Figure 7</a>: Experiment: Flan-t5-xxl vs. Dolly-v2-12b\nRecorded win rates: 0.79 vs 0.21\n<br><br>22\n<br><br>"
        }
      ]
    },
    "annotations": [
      {
        "id": "2c8b5a78-b95f-48bc-881d-96a1ad89a1e8",
        "page": 1,
        "x": 564,
        "y": 871.1875,
        "comment": "yeowwwwwwww",
        "type": "note"
      },
      {
        "id": "2c8b5a78-b95f-48bc-881d-96a1ad89a1e8",
        "page": 1,
        "x": 564,
        "y": 871.1875,
        "comment": "yeowwwwwwww",
        "type": "note"
      }
    ]
  }
]